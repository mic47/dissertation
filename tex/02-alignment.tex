\chapter{Alignments}

Parts of two sequences are homologous, if they have evolved from same part of
their ancestral sequence. Aim of sequence alignment is to identify homologous
sequences. There are several evolution events, that usually happens:
mutation of residue into another residue, deletion of part of the sequence,
insertion of residues into the sequence. We omit rearrangements of sequence,
because it cannot be represented by alignment. Alignment is data structure that
represents comparisons of sequences. We obtain alignment of $k$ sequences by
inserting dashes into sequence in a way, that they will have same length. We can
represent alignment as matrix or table. Row of the alignment is a sequence with
inserted dashes and column is list of residues from all row on same position.


Biological meaning of an alignment is following: homologous residues (those that
have developed from common ancestor) are in same row. Dashes represents either
the parts of sequence that were deleted during evolution (deletions) or that on
those positions were inserted some sequences to other sequence (insertions). In
alignment is not possible to distinguish between insertions and deletions -- we
cannot tell if something was inserted into one sequence or if there was deletion 
in the other sequences. Therefore we will refer to insertions and deletions at
to \firstUseOf{indels}.

\begin{example} 
Consider following evolution history of hypothetical DNA sequences of two
current organism $X$ and $Y$. There was parent sequence $P$ which evolves into
$X'$ and $Y'$ and after that $X'$ evolves into $X$ and $Y'$ evolves into $Y$.
This evolution history is described bellow: 
\begin{verbatim}
X:      C C     G C G A C C T T G C             A C C A
X':     C C     G           T T G C             A G C A
P:      A C T G G           T C G C T G A G C T A G C A
Y':     T C T G G           C C           G C T A G C A
Y:      T C T A G           C C           G A T A G C A
\end{verbatim}
%$X$ and $Y$ evolved from parent sequence $P$ through sequences $X'$ and $Y'$.
During evolution from $P$ to $X'$, four events occurred: deletion of 
sequences ``TG''  and ``TGAGCT'' and mutations of two bases. Evolution of $X$
was ended by  
positions bases were substituted. During evolution from $X'$ to $X$, one base was
changed and sequence ``CGACC'' was inserted.  

From evolution history described above, we can create alignment of current
sequences $X$ and $Y$ by removing ancestral sequences $X',Y'$ and $P$, 
%\begin{verbatim}
%X:      C C     G C G A C C T T G C             A C C A
%Y:      T C T A G           C C           G A T A G C A
%\end{verbatim}
%We obtain actual alignment by 
removing columns that contains only gaps and
replacing gaps with dashes (gap symbols). 
\begin{verbatim}
X:      C C - - G C G A C C T T G C - - - A C C A
Y:      T C T A G - - - - - C C - - G A T A G C A
\end{verbatim}
In this alignment symbols that are in same column are truly homologous (they
evolved from same symbol in $P$).
%If two residues are in same column in alignment then they are homologous.
As you can see, homologous symbols do not have to be equal.
\end{example}

There is only one alignment that reflects evolution history. Our goal is to find
this alignment, or at least find the alignment, that is as close as possible.
Alignment described above is \firstUseOf{global alignment} because it is
alignment of whole sequences. Local alignment is alignment of parts of
sequences: local alignment of sequences $X$ and $Y$ is global alignment of
strings $\bar{X}$ and $\bar{Y}$ where $\bar{X}$ is substring of $X$ and $\bar{Y}$ is substring of
$Y$.  Since global alignment does not consider rearrangement events
\footnote{For example duplication, reversal or translocation.}, local alignments
are useful in such scenarios.  We will mostly consider global alignments, but
most of the methods can be altered also to local alignments.

In this chapter we will review basic methods how to create alignments. We will
discuss basic scoring schemes and algorithms that finds optimal alignment under
such scheme.

%At first
%we will discuss pair alignments (alignment of two sequences).
%\correction{Later in this
%chapter we will review some methods how to construct multiple alignments.}{urvat
%ak to nie je pravda}

\section{Scoring Schemes}

Since we want to construct alignments that have biological meaning, we have to
develop a way, how to assess the quality of an alignment. On way of doing it is
to define scoring scheme. There are many ways how to develop good scoring
scheme. 


Scoring schemes covered in this chapter will score each column of an alignment,
which does not contain gap, independently. Gaps will be penalised by penalty
that depends on the length of the gap. Score of an entire alignment is sum of
the scores of all ungapped columns plus sum of the scores of all gaps. The
simplest scoring scheme is following. 

We will expect, that all sequences are from finite alphabet $\Sigma$. For DNA,
$\Sigma=\{A,C,G,T\}$, for proteins sigma contains $20$ codes of amino acids.
We will score column containing residues
$a$ and $b$ by $S[a,b]$ where $S$ is matrix of size $|\Sigma|\times|\Sigma|$.
$S$ is called \firstUseOf{substitution matrix}.
Every gap gap has score $g+dx$ where $x$ is the length of an alignment. $g$ is gap
opening penalty and $d$ is gap extension penalty. Both are usually
negative\footnote{Having positive gap penalty is probably not very useful.}.  We
call such gap model affine gap model. 

Intuition behind this scoring system are probabilistic. Assume that we have
alignment of $X$ and $Y$ without gaps and that each base in DNA evolves
independently. Therefore for every pair of residues $a,b$ there is probability
that $a$ will evolve into $b$. Therefore the probability that $X$ will evolve
into $Y$ is product of probabilities that $X_i$ will evolve into $Y_i$. By
taking logarithm we will get sum of logarithm of substitution probabilities.
Therefore values $S[a,b]$ are proportional to the logarithm of probability that
$a$ evolves into $b$. Values of $S$ are usually probabilities multiplied by some
constant and rounded to integers to avoid use of floating point numbers. 

Basically, there are two ways of deriving substitution matrices. One way is
to derive them from alignments that were constructed manually by biologists. Such
matrices are for example PAM or BLOSUM matrices. Other possible solution is to
use theoretical model of evolution, for example Jukes-Cantor model. More details
can be found in \cite{Durbin1998}.

\section{Needleman-Wunsch algorithm}

Alignment with scoring scheme with previous section can be found by
Needleman-Wunsch algorithm \cite{Durbin1998}.
\todo{nieco o tom ze to bol povodne kubicky, ale sankoff spravil lepsiu verziu}
This algorithm uses arbitrary score table $S$, affine gap model with gap penalty
$d$ with
gap opening penalty $g=0$ (this model is sometimes called linear gap model). To
align sequences $X$ and $Y$ with length $n$ and $m$ respectively, we define
matrix $M$ of size $n\times m$. $M[i,j]$ will be the score of the best alignment
of sequences $X[:i]$ and $Y[:j]$. We can compute $M[i,j]$ by following
equations:

\begin{align} 
M[0,0] &= S[X_0,Y_0]\\
M[0,i] &= i\cdot d, 0< i < m\\\
M[i,0] &= i\cdot d, 0< i < n\\
M[i,j] &= \max
\begin{cases}
 M[i-1,j-1]+S(X_i,X_j)\\M[i,j-1]+d\\
 M[i-1,j]+d
\end{cases}, 0<i<n,0<j<m \label{ALIGN:ALGO:AFFINE}
\end{align}

By computing $M[n-1,m-1]$ we have the score of the alignment of $X$ and $Y$ with
the highest score. We can reconstruct such alignment by back-tracing matrix $M$.
Back-tracing procedure will be described later.

To cope with gap opening penalty, we have to slightly change the algorithm.
We define two other matrices $M_X$ and $M_Y$ of same size as $M$. $M_X[i,j]$
will contain the score of alignment of sequences $X[:i]$ and $Y[:j]$ than ends
with gap in sequence $X$. $M_Y$ is analogous. Now we show how to compute $M,M_X$
and $M_Y$. 

\todo{Je vlastne vobec treba taketo vypisovat?}
\begin{align}
M[0,0] &= S(X_0,Y_0)\\
M[0,i] &= M_X[0,i] = i\cdot d+g, 0 < i < m\\
M[i,0] &= M_Y[i,0] = i\cdot d+g, 0 < i < n\\
M_X[i,0] &= -\infty, 0\leq i< n\\
M_Y[0,i] &= \infty, 0 \leq i< m\\
M[i,j] &= \max
\begin{cases}\label{ALIGN:ALGO:REALAFFINESTART}
 M[i-1,j-1]+S(X_i,X_j)\\
 M[i,j-1]+d\\
 M[i-1,j]+d\\
 M_X[i,j]\\
 M_Y[i,j]
\end{cases}, 0<i<n,0<j<m\\
M_X[i,j] &= \max
\begin{cases}
M[i-1,j]+g+d\\
M_X[i-1,j]+d
\end{cases}, 0<i<n,0<j<m\\
M_Y[i,j] &= \max
\begin{cases}
M[i,j-1]+g+d\\
M_Y[i,j-1]+d
\end{cases}, 0<i<n,0<j<m\label{ALIGN:ALGO:REALAFFINEEND}
\end{align}
As in previous case, values of $M,M_X$ and $M_Y$ can be computed by simple
dynamic and by back-tracing these matrices we can obtain optimal alignment of
$X$ and $Y$.  Both algorithms have time complexity of $O(nm)$ and memory
complexity $O(mn)$.  Using tricks described later, the memory and even time
complexity will be improved.

\section{Dynamic Programming}\label{DYNPROG}

In this section we show how to compute those values effectively and describe
several methods how to decrease memory footprint and/or time complexity. 

Both algorithm from previous sections contain recursive procedure, that update
matrix $M$ (or three matrices $M,M_X$ and $M_Y$). It used only values from
neighbouring cells which have at least one coordinate smaller. Let $F$ be the
function, that takes as input matrix $M$ (or $M, M_X$ and $M_Y$ in case of
$g\not=0$) and coordinate $(i,j)$ and computes
$M[i,j]$ according equation \ref{ALIGN:ALGO:AFFINE} (or equations
\ref{ALIGN:ALGO:REALAFFINESTART}-\ref{ALIGN:ALGO:REALAFFINEEND}).
Let $F'$ be same function as $F$, but with $\max$ replaced with $\arg\max$.
Clearly, $F'(M,(i,j))$ will return which cell was used to computation of
$F(M,(i,j))$.

%For simplicity, we will assume that both sequences has same length $n$. This
%will allow us to express complexity in simple way.  At first we show how
%to compute compute those algorithms in $O(n^2)$ time and $O(n^2)$ 
Needleman-Wunch algorithm can be computed by following code. Note that in case
of gap opening penalty is not zero, matrix $M$ should be replaced with triple of
matrices $(M,M_X,M_Y)$. 

\lstset{showstringspaces=false}
\begin{lstlisting}
Initialize M[i,0] and M[0,i]
for i in 1...n-1
  for j in 1...m-1
    M[i,j] = F(M,(i,j))
(i,j) = (n-1,m-1)
while i > 0
  (i',j') = F'(M,(i,j))
  (a,b) = (X[i],Y[j])
  if i' = i then a = '-'
  if j' = j then b = '-'
  print column of alignment (a,b)
  (i,j) = (i',j')
\end{lstlisting}

Lines 2-5 fills matrix $M$ and lines 6-12 implements the back-tracing procedure.
Time complexity of this algorithm is $O(nm)$ and memory requirements are $O(mn)$
since we keep in memory matrix $M$.

Note, that for computing $i$-th row of matrix $M$ we need only values from row
$i$ and $i-1$. Therefore if we want to know just score of the optimal alignment
we can compute it in $O(m+n)$ memory: after computing of row $i$, we can discard
row $i-1$.

In following subsections we will review several techniques that can be used to
improve performance of dynamic programming used to compute alignment.

\ subsection{Restricting Search Space}

The commonly used technique that is used to speedup (and decrease memory
requirements) of sequence alignment is to restrict the search space of dynamic
programming. By restricting search space we mean to compute alignment only in
some parts of matrix $M$. We assume that omitted parts of matrix correspond to
alignments with low score. In this section we will review few techniques that are
used to with local or global alignment.

If sequences are quite similar, the optimal global alignment will not be too far
from diagonal of matrix $M$. Therefore it is not necessary to compute parts of
matrix $M$, that are too far from diagonal (distance from diagonal is user
defined value). However, this is not useful for local alignment or global
alignment of too distant sequences. Now we will discuss some more advanced
techniques to restricting search space of dynamic programming.

\subsubsection{Seeds}

Seeds are usually used to reduce time complexity of local alignments.  They were
introduced in BLAST algorithm \cite{Altschul1990}.  Seed is position in both
sequences, that can be extended to alignment with high score. After is seed
found, it is extended with extension algorithm to local alignment.

Seeds are usually found by some heuristic algorithm, that computes set
\firstUseOf{candidate seeds}. After that, each candidate seed is extended into
local alignment. Those which were not extended to high-scoring alignment are
discarded. Such candidate seeds are called \abbreviation{false-positive}{FP}.
Candidate seeds that were extended to high-scoring alignments are called
\abbreviation{true-positive}{TP}.  Seeds that were not found by our heuristics
are called \abbreviation{false-negatives}{FN} and rest is called
\abbreviation{true-negatives}{TN}. It is important that heuristics should have
high sensitivity ($TP/(TP+FN)$) and high specificity ($TN/(TN+FP)$). If
heuristics have low sensitivity, many true high-scoring local alignment will not
be found. Low specificity implies longer running time. 

Traditional approach is to find candidate seeds positions $i$ and $j$ in
sequences $X$ and $Y$, such that $X[i:i+\tau]=Y[j:j+\tau]$ for some constant
$\tau$. This approach is used in BLAST \cite{Altschul1990}. Other approaches is
to allow some constant number of mismatches \cite{Kent2002}. \firstUseOf{Space
seeds} allow to contain gaps \cite{Ma2002}. \firstUseOf{Vector seeds}
\cite{Brejova2005vector} are extension of space seeds and \firstUseOf{daughter
seeds} \cite{Csuros2005} are generalization of previous seed methods. All of
those methods vary in speed, sensitivity and specificity.

Extension is done in both ways, usually using equation \ref{ALIGN:ALGO:AFFINE}
(equation is altered in reverse direction). Extension is stopped, when some
criteria is reached. For example, BLAST introduced X-drop heuristics: extension stops
if score of alignment is lower than best score that was seen so far minus user
defined constant \cite{Altschul1997}.

%space seeds, daughter seeds

\subsubsection{Stepping Stone Algorithm}

\abbreviation{Stepping stone algorithm}{SSA} was used in
\cite{Meyer2002,Pairagon2009} and it is suitable for global alignment. It uses
local alignment algorithm as subprogram. Idea of SSA is to use good local
(generated by some local alignment tool, i.e. BLAST \cite{Altschul1997})
alignment as anchors. Anchor is similar to seed: it is short alignment which we
expect to be in optimal global alignment\footnote{Or at least to be ``near''
optimal global alignment.}.  However, local alignments tools give local
alignments that do not have to be consistent with each other. Set of local
alignments is consistent if all local alignments can be together in one global
alignment.  Therefore there is need for algorithm, that will choose consistent
subset of alignments.

To get such subset $S$, SSA uses following greedy method. It start with
$S=\emptyset$. In every step it finds alignment $A$, such that $S\cup\{A\}$ is
consistent set of alignment and $A$ has highest score possible. Once there is no
such alignment, iteration will stop and $S$ contains consistent set of
alignments which will be used as anchors.

Unlike seeds, these anchors will not be extended to global alignment. Since
local alignment can contain some errors (they were generated by some faster
local alignment tool), SSA will relax them. If $X_i$ and $Y_j$ were aligned in
some anchor, then $X_i$ can be aligned to positions from $j-\tau$ to $j+\tau$ in
global alignment\footnote{Or aligned to gap in that region.} for user defined
constant $\tau$. Similarly, $Y_j$ can be aligned to positions from $i-\tau$ to
$i+\tau$.

Time complexity and memory requirements of stepping stone algorithm are of order
of $O(\sqrt{|X|^2+|Y|^2})$ \cite{Meyer2002}.

%At first it runs
%local alignment tool, like BLAST \cite{}, to get set of good local alignments
%(local alignment is good if it's score and $p$-value is higher than some
%threshold).  Intention is to use good local alignments as anchors: we will
%search only for global alignments that are near such local alignments. However,
%local alignments do not have to be consistent. 

%Therefore we have to choose subset of alignments that we will used as anchors.

%Meyer {\it et al. (2002)} gave following greedy algorithm: The sort local
%alignments according their score. They will use the 
%They start with empty set of anchors. At every
%step take take  


\subsection{Checkpointing}

Checkpointing is general trick in dynamic programming, that allows us to save
$O(\sqrt n)$ rows of dynamic programming matrix while doubling running time
\cite{Grice1997}. 
%We will treat matrices $M,M_X$ and $M_Y$ as one matrix
%containing triples.

In order to compute $i$-th row of matrix $M$, we need only row $i-1$. As
mentioned above, to compute score of the best alignment we need to remember only
two rows (previous and current which we are computing).  If we want to
back-trace the optimal alignment, after we compute it's score, we need all
rows of matrix $M$ again, therefore we would have to compute them again.
Since we need for back-tracing rows in decreasing order, this would lead to
$O(m^2n^2)$ time, since every time we need previous row we would have to recompute
it from beginning.

Checkpointing solves this problem by remembering every $k$-th row of matrix $M$.
For this we need to remember $\lceil n/k\rceil$ rows.  While back-tracing, we
will remember additional block $B$ of consecutive $k$ rows in interval
$<ik,(i+1)k-1>$. We can compute such block on $O(kn)$ time using simple
non-optimized dynamic programming described above.  If we need row that
is outside $B$, we replace block $B$ with block that contain such row. Since we
access every row only once and in decreasing order, we recompute every block at
most once. Therefore time complexity of back-tracing will be still $O(n^2)$. We
have to remember every $k$-th row and one block of size $k$, so memory
complexity is $O(\lceil n/k\rceil n+ kn)$. If we set $k=\sqrt n$ then memory
complexity will be $O(n\sqrt n)$ while time complexity will remain same.


 
\subsection{Hirschberg Algorithm}

\abbreviation{Hirschberg algorithm}{HA} is divide and conquer approach to reduce
memory requirements of sequence alignment \cite{Hirschberg1975}. Idea is
following: if we want alignment of sequences $X$ and $Y$ which has bases $X_i$
and $Y_i$ aligned, we have do dynamic programming only in submatrices
$M_1=M[0:i-2,0:j-2]$ and $M_2=M[i-1:n-1,j-1:m-1]$. If $i=\lceil n/2\rceil$ then
total number of cells in those matrices is roughly half of the number of cells
in $M$. Hirschberg algorithm incorporates procedure how to compute such $j$ for
$i=\lceil n/2\rceil$ in optimal alignment of both sequences in $O(nm)$ time and
$O(n+m)$ memory\footnote{There is one problem. $X_i$ does not have to be aligned
to $X_j$, because $X_i$ is indel. In such case, there is $j$ such that $X_i$ is
aligned to gap that comes right after $X_j$ and we have to change $M_1$ to
$M[0:i-2,0:j-1]$}. HA use such $j$ to determine submatrices $M_1,M_2$. Optimal
alignment is concatenation of optimal alignments in matrices $M_1$ and $M_2$.

To determine $j$, such that $X_i,i=\lceil n/2\rceil$ is aligned to $Y_j$ (or
$X_i$ is aligned to gap right after $Y_j$) HA uses following algorithm: Let
$B(X,Y)$ be the algorithm, that computes for $X$ and $Y$ vector $LL(k)$, where
$LL(k)=M[n-1,k]$ ($LL$ is the last row of $M$). This can be computed in
$O(nm)$ time and $O(n+m)$ memory using algorithm from section
\ref{DYNPROG}.  We compute $LL_1=B(X[0:i],Y)$ and $LL_2=B( (X[i,n])^R,Y^R)$.
While $LL_1[k]$ contains score of optimal alignment of $X[0:i]$ and $Y[0:k]$,
$LL_2^R[k]$ contains score of optimal alignment of $X[i,n]$ and $Y[k,m]$.
$j$ is such column, that maximizes $\max\{LL_1[j]+d+LL_2^R[j],
LL_1[j-1]+LL_2^R[j] \}$.

Since total size of the subproblem is always at most half of the size of the
original problem, the running time of the algorithm will double and therefore
running time of HA is still $O(|X||Y|)$. HA keeps in memory only constant number
of rows of $M$ and reconstructed alignment and therefore memory requirements are
$O(|X|+|Y|)$.

Hirschberg Algorithm (HA) reduce memory more than checkpointing, but HA can be
only applied to algorithms that use back-tracing. For example Forward-Backward
algorithm can be improved by checkpointing but not by HA. More in chapter
\ref{CHAPTER:HMM}.

 
\subsection{Exploiting Sequence Repetition}

This technique reduce time complexity of alignment algorithm to $O(n^2/log n)$.
Unlike four-russian trick \cite{GusfieldBook}, this technique enables usage of
any cost matrix.  This idea combines LZ78 factorization \cite{Lempel1976} and
$O(A+B)$ algorithm for computing row minima/maxima in totally monotone matrix of
size $A\times B$ \cite{Aggarwal1987}. We will discuss only two main ideas behind
this algorithm.

\subsubsection{Totally Monotone Matrices}

\begin{definition}\cite{Crochemore2002}
Matrix $A$ of size $n\times m$ is \firstUseOf{totally monotone} (with concave condition),
if and only if for all $0\leq i,j< n, 0\leq k<l<m$ following condition holds:
if $M[i,k]\leq M[j,k]$ then $M[i,l]\leq M[j,l]$.
\end{definition}

If $M$ is totally monotone then we can compute maximum of every row (or column)
in $O(n+m)$ time by SMAWK algorithm \cite{Aggarwal1987} (This problem is called
\firstUseOf{row maxima}).

\begin{definition}\cite{Crochemore2002}
Let $M$ be matrix and $M'$ be it's
submatrix. \firstUseOf{Input border} $I_{M'}$ of $M'$ is the left and top
borders of $G$ and \firstUseOf{output border} $O_{M'}$ of $M'$ is the right and
bottom border of $M'$. Elements of $I_{M'}$ are ordered in clockwise direction
and elements of $O_{M'}$ are ordered in counter-clockwise direction.
\end{definition}

Intuition behind input and output border is following. If we look on the
computation of alignment algorithm inside submatrix $M'$, input border is input
to this computation and output border is output of this computation.

\begin{definition}\cite{Crochemore2002}
Let $M'$ be submatrix of $M$, $I_{M'}=\{i_0,i_1,\dots,i_{k-1}\}$ be its input
border, and $O_{M'}=\{o_0,o_1,\dots,o_{l-1}\}$ be its output border then
matrices
$DIST$ and $OUT$ of size $k\times l$ are defined in following way:
$DIST[a,b]$ is the cost of optimal alignment from cell $I_a$ to cell $I_b$.
$OUT[a,b]=I_a+DIST[a,b]$.
\end{definition}

Clearly, $O_b=\max_{0\leq a < k}OUT[a,b]$. Therefore by computing row maxima in
matrix $OUT$, we can compute values of output borders. Matrices $DIST$ and $OUT$
are totally monotone \cite{Crochemore2002}.  If we have matrices $DIST$ and
$OUT$ in advance and $M'$ is of size $m'\times n'$ then we can compute values of
output border in time $O(n'+m')$ by SMAWK algorithm.

Now we show how to divide matrix $M$ into number of submatrices in a way, that
it is possible to effectively represent matrices $DIST$ and $OUT$.

%Let $M$ be dynamic programming matrix for alignment algorithm for some sequences
%$X$ and $Y$. Let $M' =
%M[i:j,k:l]$ be rectangular submatrix of $M$. Values of $M'$ depends on 
%values cells in submatrices $M[i-1,j:l]$ and $M[i:k,j-1]$ and on value
%$M[i-1,j-1]$. Let $I_{M'}$ be the set of such cells. Let $O_{M'}$ be the 
%the set of cells in $M[i-1,j:l]$ and $M[i:k,j-1]$ (the cells in the top row or
%right column of $M'$). Let $I_0,I_1,\dots$ be enumeration of cells in $I_{M'}$
%and $O_0,O_1,\dots$ be enumeration of cells in $O_{M'}$.  Now we will construct
%the  matrix $DIST_{M'}$ of size $|I_{M'}|\times|O_{M'}|$ where  
%$DIST_{M'}[i,j]$ is the cost of optimal alignment from cell $I_i$ to cell $O_j$.
%Let $OUT[i,j]=I_i+DIST[i,j]$. Both $DITS$ and $OUT$ are totally monotone with
%convex condition 

\subsubsection{LZ78 factorization}

LZ78 is compression algorithm that uses dynamic dictionary that is being built
while sequence is compressed \cite{Lempel1976}. The way how it parse sequence
can be used to accelerate dynamic programming \cite{Crochemore2002,Weimann2009}. We will be interested in
factorization of input sequence by this algorithm. LZ78 factorization divide
sequence $S$ into $k$ strings $S_0,\dots,S_{k-1}$, where $S_0S_2\dots S_{k-1}=S$ and
for every index  $i,0< i <k$ there is index $0\leq j<i$ such that $S_j$ is
prefix of $S_j$ of size $|S_i|-1$. We will call $S_j$ to be a
\firstUseOf{predecessor} of $S_i$.  We have guarantee, that number of strings
$k$ is in  $O(\frac{n}{\log n})$ where $n$ is the length of $S$
\cite{Lempel1976}. 

To align sequences $X$ and $Y$, we factorize them into sequences of strings
$\{X_i\}_{0\leq i < k_s}$ and $\{Y_i\}_{0\leq i<k_t}$.  Every pair $(X_i,Y_j)$
define pair of rectangular block $B_{i,j}$ submatrix of $M$.  All $B_{i,j}$ are
disjoint and all blocks cover matrix $M$. We say that block $B_{k,l}$ is
predecessor of block $B_{i,j}$ if one of the following conditions is true:

%If we want to align sequences $S$ and $T$, we factorize $S$ and $T$
%into sequences of strings $\{S_i\}_{0\leq i < k_s}$ and $\{T_i\}_{0\leq i<
%k_t}$. Every pair $(S_i,T_j)$ defines rectangular block of matrix $M$.
%We say that $(S_k,T_l)$ is predecessor of $(S_i,T_j)$ if one of the following
%conditions is true:

\begin{itemize}
\item $S_k$ is predecessor of $S_i$ and $l=j$
\item $k=i$ and $T_l$ is predecessor of $T_j$
\item $S_k$ is predecessor of $S_i$ and $T_l$ is predecessor of $T_j$
\end{itemize}

Note, that every block can has only $0,1$ or $3$ predecessors. Only block
$B_{0,0}$ will have $0$ predecessors and only blocks $B_{i,0}$ and $B_{0,i}$
($i>0$) have only $1$ predecessor.  

\todo{Mozno by nebolo odveci popisat aj tu strukturu, ak bude cas}
{\it Crochemore et al.} described data structure that represents $DIST$ and $OUT$
matrices for block \nocite{Crochemore2002} $B_{i,j}$. That structures can be
computed from $DISTS$ and $OUT$ matrices of theirs predecessors in time
$O(|X_i|+|Y_j|)$. Details about this algorithm can be found in
\cite{Crochemore2002}. Time complexity of this algorithm if proportional to the
sum of the sizes of all blocks, which is $O(n^2/\log n)$ where $n$ is the length
of the longer sequence.

\bigskip
{\large\bf Odtialto hore povazujem vsetko za ``hotove''. }
\bigskip



\section{Non-Affine Gap Models} 

Reason why affine gap models are used in sequence alignment is because they are
simple, easy to compute and gave reasonable results. While affine gap score
works fine for short gaps, penalty for long gaps is too high. Use of different 
gap models can improve alignments \cite{Gill2004,Cartwright2009}.

Let $f(x)$ be the gap penalty where $x$ is the length of the gap. In case of
affine gap models, $f(x)=g+dx$. In case of general gap function, we have to
reformulate equation \ref{ALIGN:ALGO:AFFINE} in following way:
\begin{align}
M[i,j] &= \max
\begin{cases}
 M[i-1,j-1]+S(X_i,X_j)\\
 M[i,j-y]+f(y)\\
 M[i-x,j]+f(x)
\end{cases}, 0<x\leq i<n,0<y\leq j<m
\end{align}
This algorithm has time complexity of $O(n^3)$ where $n$ is the length of the
longest sequence. Note that this was the original Needleman-Wunsch\cite{}
algorithm and later ?? improved algorithm for affine gap functions\cite{}.
Unlike previous recursive equations, int this algorithm $M[i,j]$ depends not
only on neighbouring cells, but also on all previous in same row and column.
Therefore some techniques like Hirschberg algorithm or Checkpointing cannot be
used with general gap penalties.

\subsection{Convex/Concave Gap Functions}

Arbitrary gap penalties leads to slow running time, however if we place some
restrictions on gap penalty, we can use faster algorithms. One such restriction
is if gap penalty is convex or concave.

\begin{definition}
Function $f(x)$ is 
\end{definition}

Now we describe algorithm to compute best alignment with convex gap function.
Function $f(x)$ is convex, if and only if

\[f(x+y)\leq f(x)+f(y)\]
 

%tu chcem definiciu convexnej gap funkcie
%algoritmus ktory je v case n^2 log(n)
%algoritmus, ktory bezi v case n^2 alpha(n)
%kubicky algoritmus v worstcase, ale prakticky konstantny (budem musiet najst 
%citaciu -- nepodarilo sa mi to najst. Zaujimave\ldots)



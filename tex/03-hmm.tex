\chapter{Hidden Markov Models}

\abbreviation{Hidden Markov Models}{HMM} are graphical probabilistic model
commonly used to sequence annotation or sequence annotation. HMM is
probabilistic finite state machine that in every state emit one symbol. There
are variants of HMMs that emits more symbols or which emits symbols on multiple
tracks, we will discuss them later. In this chapter we will describe basic
definitions and algorithms that are used with HMMs.

\section{Definitions}
                       
%HMM
%Pravdepodobnost
%Posterior pravdepodobnost
%Anotacia
%Pravdepodobnost anotacie
%Footprint
HMMs are generative probabilistic models (formal definition is given below). Initially HMM is in random state $q$
according to \firstUseOf{initial distribution} $I$.  When HMM is in arbitrary
state $q$ it emits one symbol from some alphabet $\Sigma$ according distribution
$e_q$ and move to another state according transition distribution $a_q$. Note
that emission and transition distribution can be different for every state.
This process produce two sequences: sequence of states
$\pi=\pi_0\pi_1\pi_2\dots$ called \firstUseOf{state path} and output sequence
$X=X_0X_1X_2\dots$ over alphabet $\Sigma$. In this work we will use only
discrete versions of HMMs and state space and alphabet will be always finite.
HMMs can have several final states. If final states are present, once HMM reach
that state generation of a sequence stops. In this chapter we will ignore final
states. \todo{Budeme ich vobec niekde potrebovat?}

\begin{definition}
A tuple $H=(\Sigma,V,I,e,a)$ is \abbreviation{Hidden Markov Model}{HMM} where
$\Sigma=\{\sigma_0,\dots\sigma_{m-1}\}$ is finite alphabet of size $m$,
$V=\{v_0,\dots,v_{k-1}\}$ is finite set of state of size $k$. $I$ is
distribution over $V$, $e$ is $k\times m$ matrix where each row contains
distribution over $\Sigma$ and $a$ is stochastic matrix of size $m\times m$.

We will write members of $e$ and $a$ as subscript, therefore $e_{u,v}$ is
element from $e$ in $u$-th row and $v$-th column.  

Therefore following conditions holds:
\begin{enumerate}
\item $\forall v\in V, I_v \geq 0$
\item $\sum_{v\in V}I_v=1$
\item $\forall u\in V,\forall \sigma\in\Sigma, e_{u,\sigma}\geq0$
\item $\forall u\in V, \sum_{\sigma\in \Sigma}e_{u,\sigma}=1$
\item $\forall u,v\in V, a_{u,v}\geq0$
\item $\forall u\in V, \sum_{v\in V}a_{u,v}=1$
\end{enumerate}
\end{definition}

The conditions $1-6$ just ensure, that everything is correct probabilistic
distribution.

\begin{example}
\end{example}

\begin{definition}
Let $H=(\Sigma,V,I,e,a)$ be an HMM.
We say that there is \firstUseOf{transition} from state $u$ to state $v$ if
$a_{u,v}>0$. We will transition from $u$ to $v$ as $u\to v$ and $T=\{u\to v\mid
a_{u,v}>0\}$ is the set of all transitions of $H$.

\firstUseOf{State path} $\pi=\pi_0\pi_1\dots\pi_{n-1}$ is the sequence of
states. We say that state path $\pi$ is \firstUseOf{admissible} if $I_{\pi_0}>0$
and for all $1\leq i < n$ $\pi_{i-1}\to\pi_i\in T$. Otherwise $\pi$ is
\firstUseOf{inadmissible}.
\end{definition}

\begin{example}
\end{example}

%Formally, Hidden Markov Model is defined by it's finite state space
%$Q=\{q_0,q_2,\dots, q_{k-1}\}$ of size $k$, initial probabilistic distribution
%$I$ over state space, finite alphabet $\Sigma = (\sigma_0, \sigma_1, \dots,
%\sigma_{m-1})$ of
%size $m$, emission and transition distribution over $\Sigma$ and $Q$
%respectively defined for every state independently. We denote emission
%distribution of state $q$ by $e_q$ and transition distribution by $a_{q}$. 

\begin{definition}
Let $H=(\Sigma,V,I,e,a)$ be a HMM and $X=X_0X_1\dots X_{n-1}$ be sequence over
alphabet $\Sigma$ of length $n$. Let $\pi$ be state path of length $n$. Then the
probability that state path generated sequence $X$ is 

\[\Pr\left(X,\pi\mid H\right) =
I_{\pi_0}e_{\pi_0,X_0}\prod_{i=1}^{|X|-1}e_{\pi_i}a_{\pi_{i-1},\pi_i}\]

If length of the sequence $X$ is not equal to length of the state path $\pi$, that
probability that $\pi$ generates $X$ is zero.
\end{definition}

\begin{example}
\end{example}

\begin{definition}
Let $H=(\Sigma,V,I,e,a)$ be a HMM and $X=X_0X_1\dots,X_{n-1}$ be sequence over
alphabet $\Sigma$ of length $n$. The probability that $X$ was generated by the
model $H$ is 
\[\Pr\left(X\mid H\right)=\sum_{\pi\in V^n}\Pr\left(X,\pi\mid H\right)\]
\end{definition}

%The probability of emission of sequence $X$ with state path $\pi$
%is \[\Pr\left(X,\pi\mid H\right) =
%I(\pi_0)e_{\pi_0}\left(X_0\right)\prod_{i=1}^{|X|-1}e_{\pi_i}a_{\pi_{i-1}}\left(\pi_i\right)\]
%Generally, there are many ways how to generate every sequence.  Therefore every
%HMM $H$ defines probability distribution \[\Pr\left(X\mid H\right) =
%\sum_{\pi,|\pi|=|X|}\Pr\left(X,\pi\mid H\right)\].  Since in every step HMM
%generates one symbol from $\Sigma$ and one state, state path has same length as
%generated sequence.

\begin{note}
In this definition of HMM, the sum the probabilities of all sequences of length
$n$ is $1$.
\end{note}

\todo{Tento text chcem dat asi trosku neskor, Az ked to budem potrebovat}
We will abuse the notation and by $e_q(\cdot)$ we mean row vector
$\left(e_q(b_1),\dots,e_q(b_{\sigma})\right)$. Similarly,
$a_q(\cdot)=\left(a_q(q_1),\dots,a_q(q_k)\right)$ is row vector that contain
transition probabilities from the state $q$.  $a_q(\cdot)$ have therefore
dimension $1\times k$. By $A$ we will denote the $|Q|\times |Q|$ matrix
consisting from vectors $a_{q_1},a_{q_2},\dots,a_{q_k}$.  Therefore $A[q_i,q_j]$
contains probability of transition from state $q_i$ to $q_j$.

\section{Forward Algorithm}
Forward algorithm computes for given sequence $X$ of length $n$ the quantity $\Pr\left(X\mid
H\right)$, the probability that sequence
was generated by the model. 
\begin{eqnarray}
F[0,v] &=& I_ve_{v,X_0}, v\in V\\
F[i,v] &=& \sum_{u\in V}F[i-1,u]*a_{u,v}*e_{v,X_i}, v\in V,0< i < n
\end{eqnarray}
We call values $V[i,v]$ forward variables. Value $V[i,v]$ is the
probability, that $X[:i+1]$ was generated by state path that ends with state
$v$. Therefore \[\Pr\left(X\mid H\right) = \sum_{v\in V} V[|X|-1,v]\]

Using recursive equations above, we can compute the probability of the $X$ in
$O(nm^2)$ time and $O(m)$ memory by dynamic programming. Forward variables are
also used in other algorithm. Storing all of the forward variables requires
soring $O(nm)$ numbers.

\section{Sequence Annotation}

%definujeme annotation, footprint, set of colors

One way of using HMMs is to use them to sequence annotation. By sequence
annotation we mean assigning ``labels'' to parts of the input sequence according
to their meaning. For example in gene-finding domain, we want to assign gene
labels to those parts of the sequence, that encodes proteins. In domain of
annotation of transmembrane proteins, we want to know, which parts of the
sequence are on which  side of the membrane. In recombination prediction we want
to predict which parts of the sequence belong to which subtype of organism.

In practice, we have finite set of labels $C=\{c_0,c_1,\dots,c_{l-1}\}$ and we
want to assign one label to every symbol of input sequence. We do it by
assigning one label to every state of HMM in a way, that states of HMM that
encodes same meaning have same color. After that we try to predict the
annotation of the sequence, for example by finding the most probable state path
that can generated models and we annotate each symbol of the sequence according
to the label assigned to state that generated that symbol. We can formalized
this in following way.

\todo{Aby nebol bordel medzi label a color}

\begin{definition}
Let $H=(\Sigma,V,I,e,a)$ be HMM and $C=\{c_0,c_1,\dots,c_{l-1}\}$ be the finite
sets of labels (or colors). Then \firstUseOf{coloring funcion} 
$\lambda: V^*\to C^*$ is function, that satisfies following properties:
\begin{enumerate}
\item $\lambda(v)\in C$ for all $v\in V$.
\item $\lambda(xy) = \lambda(x)\lambda(y)$ for all $x,y\in V^*$.
\end{enumerate}

Let $X$ be sequence generated by state path $\pi$. Then annotation
$\Lambda$ of sequence $X$ is $\Lambda = \lambda(\pi)$.
\end{definition}

One problem in sequence annotation is, that usually we do not know the state
path that generated sequence. Therefore we have to use methods, that somehow
reconstruct the state path or at least give good approximation of the correct
annotation. 

\begin{example}
\end{example}

%Since many states of the HMM can have assigned same label, there may be may be
%many state path with same annotation.

\begin{definition}
Let $H$ be an $HMM$, $X$ be a sequence $\Lambda$ be a annotation of sequence
$X$. The probability of annotation $\Lambda$ given sequence $X$ if length $n$ is 
\[\Pr\left(\Lambda\mid X,H\right)=\sum_{\pi \in V^n,\lambda(\pi) =
\Lambda}\Pr\left(\pi\mid X,H \right)\]
\end{definition}

\begin{example}
\end{example}

\section{Viterbi Algorithm}
\todo{Viterbi variable ma rovnake oznacenie ako mnozina stavov. Treba to
upravit}
Viterbi algorithm is probably the most used decoding algorithm for Hidden Markov
models
Viterbi algorithm answer straightforward question: given the sequence
$X=X_0X_1\dots X_n$, what
is the most-likely state path $\pi$ that generates $X$? Formally, Viterbi
algorithm finds a state path maximizing $\Pr\left( \pi\mid X,H \right)$. Since
\[\Pr\left(\pi\mid X,H\right) = \frac{\Pr\left(\pi,X,\mid
H\right)}{\Pr\left(X\mid H\right)}\] and quantity $\Pr\left(X\mid H\right)$ is fixed
this is same as finding $\pi$ maximizing $\Pr\left(X,\pi\mid H\right)$. 

Viterbi algorithm is very similar to Forward algorithm. It starts with computing
Viterbi variables $V[i,v]$. $V[i,v]$ stores the probability of the most probable 
state path that generated $X[:i+1]$ and ends in state $v$. Viterbi algorithm
also computes back-links $B[i,v]$, which contains the previous state in the most
probable state path that generated $X[:i+1]$ and ends in state $v$. We can
compute these values by following equations:
\begin{eqnarray}
V[0,v] &=& I_{v}e_{v,X_0}, v\in V\\
V[i,v] &=& \max_{u\in V} V[i-1,u]a_{u,v}e_{v,X_i}, v\in V,0<i<n\\
B[i,v] &=& \arg\max_{u\in V} V[i-1,u]a_{u,v}e_{v,X_i}, v\in V,0<i<n
\end{eqnarray}
\begin{note}
We do not specify how $B[0,v],v\in V$ is defined since we will not
need it to be defined in our computation.
\end{note}
\begin{note}
If we compare Viterbi algorithm with the Forward algorithm, we see that
Viterbi equations are forward equations if we replace summation with
maximization.
\end{note}

Variable $V[n-1,v]$ contains the probability of the most probable state path
that generated $X$ and end in state $v$. Therefore the state $v_{\max} =
\arg\max_{v\in V}V[n-1,v]$ is the last state of the most probable state path.
Variable $B[n-1,v_{\max}]$ contains the previous state of the most probable
state path. By traversing back through back-links  we can reconstruct the most
probable state path.

\begin{example}
\end{example}

V

\section{Backward algorighm}
\section{Forward-backward algorithm}
\section{Training}
\section{Variants of HMMs}
\section{Other Decoding Methods}

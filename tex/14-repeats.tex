\chapter{Alignment with Tandem Repeats} \label{CHAPTER:REP}

A \firstUseOf{Tandem repeat} is a region in the genomic sequence that contains
two or more consecutive repetitions of some \firstUseOf{motif}, which is a
short genomic sequence. The instances of the motif in a tandem repeat are
called \firstUseOf{repetitions}.  Tandem repeats, like other sequences, undergo
evolution, and therefore events like mutations, insertions, and deletions
occurs in the individual repetitions.  Therefore individual repetitions are
only approximate copies of a motif.  More than $2\%$ of the human genome is
covered by short tandem repeats, and they occur in many genes and regulatory
regions \cite{Gemayel2010}. Additionally, recent short insertions in the human
genome are mostly caused by tandem duplication \cite{Messer2007}. Most of the
tandem repeats have evolved by tandem segmental duplications, which means that
a tandem repeat was created by several successive duplication events. The
consequence is that the homologous tandem repeats in two related sequences can
contain a different number of copies of the original motif. Note that it is
possible that none of the current repetition is the exact copy of the original
motif. 
\begin{example}
Below is an example of a sequence with tandem repeat with motif $AT$ and $12$
repetitions.  One repetition contains insertion (I in the annotation) and other
repetition contain mutation (M in the annotation). The repetitions are
annotated by alternating . and \_ characters.
\begin{verbatim}
Sequence:   ACGTCGATGCATATATATATATATAATATCTATATGAGCTGATGCTAGCTAC
Annotation:           __..__..__..__I..__M.__..
\end{verbatim}
Note that while it is not clear to which repetition the insertion belongs, we
do not find it important.
\end{example}

Aligning homologous tandem repeats is hard, because it is not clear which
repetitions are orthologous (which originate in the same ancestral repetition).
Tandem repeats do not only affect quality of alignment within them, but the
error can spread into adjacent columns of an alignment, as we show in Section
\ref{SECTION:REPSIMRESULTS}, Figure \ref{FIGURE:SFF_GRAPHS}.  These
inaccuracies can spread further and cause artifacts in the results of
comparative genomic methods that use sequence alignments.

In this chapter, we introduce a tractable model that explicitly accounts for
tandem repeats. We use the maximum expected gain framework to explore several
decoding criteria for our model. We evaluate our model using simulated data.

\section{Related work}\label{SECTION:REPALNMETHODS}

Alignment methods that take into consideration tandem duplications were first
studied by Benson \cite{Benson1997}, who proposed extension of the
Needleman-Wunsch algorithm. They extend scoring scheme by adding duplication
event, which creates tandem repeat. Duplication consists from
the duplication of the substring $U$ from one input sequence into multiple
tandem copies of $U$ into the second sequence. The score of the duplication
event was the sum of the duplication initiation cost, duplication extension
cost (multiplied by the number of repetitions), and the maximum alignment score
for aligning the sequence $U$ to its repetitions in the second sequence. Time
complexity of this algorithm is $O(n^4)$, and Benson also proposed a heuristic
algorithm to compute the alignment in a reasonable time.  Several variants of
this problem were also studied in \cite{Sammeth2006, Berard2006}. Other
possibility is to use a lossy compression scheme to collapse tandem repeats,
and then align compressed sequences \cite{Freschi2012}.

A Traditional approach to deal with tandem repeats is to mask them in both
sequences and then align masked sequences by some alignment algorithm.  Masking
means replacing tandem repeats and other low complexity regions either with
lower-case letters (soft-masking) or with N symbols (hard-masking, N represents
any base). Masking is done by some method for finding tandem repeats, such as
\abbreviation{Tandem Repeat Finder}{TRF} \cite{Benson1999}, TANTAN
\cite{Frith2011}, mreps \cite{Kolpakov2003}, or ATRhunter \cite{Wexler2005}. 

Methods mentioned above were not based on probabilistic models. The first
probabilistic method specifically targeting tandem repeats was introduced by
Hickey and Blanchette \cite{Hickey2011}.  They developed a context-sensitive
model based on pair Tree-Adjoining grammars.  Their model does not explicitly
model an arbitrary number of repeats; it focuses on short context-sensitive
indels caused by tandem duplications. Each such context sensitive indel was
modeled by assuming that the indel sequence and previous/following aligned pair
of sequences evolved from single common ancestor. The time complexity of their
decoding algorithm is $O(n^2L^2)$, where $n$ is the length of the sequences and
$L$ is the maximal length of context sensitive indels.

Another, partially probabilistic method was developed by Kováč {\it et al.
(2012)}\nocite{Kovac2012}. The aim of the method was to align repetitive motifs
occuring in some protein families (for example zinc finger proteins), which are
similar to tandem repeats in DNA. Their method focused on correctly aligning
individual occurrences of the motifs. They combined a profile HMM of the motif
and a pair HMM, and developed a new decoding algorithm similar to the Viterbi
algorithm. Despite the use of probabilistic models, their method was not a
probabilistic model, because scores from different models were combined in an
ad-hoc manner. 

Hudek \cite{Hudek2010} developed an algorithm which combines the Viterbi and
the Posterior decoding. The goal of the algorithm was to reduce misalignments due
to short tandem repeats with the motif length $1$, for example $AA$ in the
first sequence and $AAAAAAAAA$ in the second sequence. 
The algorithm does not search for alignment, but for the most probable
segmentation; alignment is divided into segments, such that each block contain
gaps in only one sequence. Within one block, algorithm considers all possible
evolutionary histories of such block; 
the probability of the block is the sum
of the probabilities of all alignments of sequences within block. The
probability of the segmentation is the product of probabilities of all blocks
along with transition probabilities between blocks. This was implemented using
pHMM with two colors (each color for one segment) and the algorithm effectively
search for the most probable annotation (segmentation). Authors give the
algorithm for searching for the most probable segmentation in $O(n^4)$ time,
where $n$ is the length of the sequences. Additionally, authors provide
heuristic with time complexity $O(n^2k)$, where $k$ is the limit for the
maximal block length.

\section{Finding Tandem Repeats}
Although our main goal is to align sequences with tandem repeats, our model
will feature submodels that can be used to find the repeats in a single
sequence. In this section, we describe our repeat submodel and repeat model
from the TANTAN repeat finder \cite{Frith2011}. We start by describing
non-probabilistic approach to finding repeats, which we also use in the
preprocessing phase of our method.

\subsection{Tandem Repeat Finder}

Probably the most popular method for finding tandem repeats is the Tandem
Repeat Finder (TRF) \cite{Benson1999}.  Tandem repeat finder finds the position
of a tandem repeat (referred as an \firstUseOf{interval}), consensus sequence
(an approximation of the original motif), alignment of the consensus sequence
and the input sequence, and various other information about each repeat.

The method consists of two components: detection and analysis. The detection
component tries to find a set of candidate tandem repeats by analyzing the
distances between occurrences of the same $k$-tuple (substring of length $k$).
It uses several statistical criteria to detect repeats and distinguish between
tandem repeats and non-tandem repeats \cite{Benson1999}.

The analysis component aligns a candidate consensus using wraparound dynamic
programming \cite{Myers1989} with the surrounding sequence. If the alignment is
not successful (candidate consensus has to be aligned at least twice to be
successful), candidate repeat is discarded. Otherwise, the final consensus
sequence is computed from the alignment along with other statistics about
tandem repeat.

Tandem repeats found by the TRF are often redundant.  They can overlap, have slightly
different consensus motifs, the motifs can be shifted cyclically, and can have
different lengths, as in the following example:

\begin{verbatim}
Sequence  X:        CACCGCCACCACCGTAG
Consensus ACCACC:    ACCACCACCACC      2 repetitions
Consensus AAC:       ACCACCACCACC      4 repetitions
consensus CAC:      CACCACCACCACC      4.3 repetitions
\end{verbatim}

The sequence $X$ contains a tandem repeat and there are three consensus
sequences with different number of repetitions. Note that the repetition of the
consensus can be incomplete; as with the $CAC$ consensus where we have $4.3$
repetitions ($0.3$ stands for the last $C$ in the repetitive sequence).  Note
that the purpose of this example to illustrate possible redundancies in the TRF
output.  If we actually run TRF on sequence $X$, the output would be only the
repeat with consensus $CAC$.

\subsection{Sunflower Model}\label{SECTION:SUNFLOWERMODEL}
The \firstUseOf{sunflower model} is an HMM that represents tandem repetious of
one previously specified motif $C=c_0\dots c_{k-1}$. We have developed this
model as a part of our model for aligning sequences with tandem repeats, which
we describe in section \ref{SECTION:REPMODELS}, but it can be also used as a
simple tool for finding tandem repeats and we use it to improve a candidate set
of tandem repeats initially constructed by the TRF program (see Section
\ref{SECTION:REPOPT}).

The sunflower model is an extension of the profile HMM described in Section
\ref{HERD:METHODS} (see Figure \ref{FIGURE:PROFILEHMM} on page
\pageref{FIGURE:PROFILEHMM}). In particular, the sunflower model is a circular
version of the profile HMM with match states representing motif $C$. This model
implicitly assumes that repetitions evolved from motif $C$ independently of
each other, as of at single point of time, motif $C$ was copied several times
forming tandem repeat, which then underwent several simple evolution events (
substitutions, deletions and insertions). This does not accurately reflect a
typical evolution history of a tandem repeat, where individual repetitions
arise over time from existing copies which could differ from the original
consensus $C$. However, trying to model such evolution would lead to a much
more complicated model.

\begin{figure}
\begin{center}
\begin{subfigure}[b]{0.5\textwidth}
\includegraphics{../figures/SunflowerSilentCircle.pdf}
\caption{Cyclic profile model}\label{SUBFIGURE:SUNFLOWERSILENTCYCLE}
\end{subfigure}%
\begin{subfigure}[b]{0.5\textwidth}
\includegraphics{../figures/Sunflower.pdf}
\caption{Sunflower model}\label{SUBFIGURE:SUNFLOWER}
\end{subfigure}
\end{center}
\caption[Example of the Sunflower model]{Example of the Sunflower model 
with the motif of length $10$. White states emit one character and gray states are
silent states. States $s$ and $e$ are initial and final silent states. Sunflower model
has additional delete states $D'_0, \dots D'_8$ to remove silent cycle from the
model.}
\label{FIGURE:SUNFLOWERMODEL}
\end{figure}

Our model contains typical profile HMM states $M_0,\dots, M_{k-1}, I_{0},
\dots, I_{k-1}$ and $D_{0}, \dots, D_{k-1}$. The transitions between the states
are similar to profile HMM:  $M_{i}\to M_{i\oplus 1}, M_i\to I_i, M_i\to
D_{i\oplus 1}, I_i\to I_i, I_i\to M_{i\oplus 1}, I_i\to D_{i\oplus 1}, D_{i}\to
D_{i \oplus 1}, D_{i}\to M_{i\oplus 1}, D_{i}\to I_i$ for all $0\leq i < k$,
where $\oplus$ is $+$ modulo $k$. As in the profile HMM, states $D_i$ are
silent. We add silent start state $s$ and silent final state $e$ with
transitions $s\to M_0, s\to D_0, D_{k-1}\to e, M_{k-1}\to e$ and transition
$s\to e$ to model empty tandem repeats. The whole model topology is shown in
Figure \ref{SUBFIGURE:SUNFLOWERSILENTCYCLE}.

The problem is that the cyclic profile contains a cycle of silent states, which
causes problems with training and decoding algorithm (see Section
\ref{SECTION:SILENT}. Removing these states would lead to additional
$\theta(k^2)$ edges. Therefore, we have decided to remove the transition
between delete states $D_{k-1}$ and $D_0$. To compensate for the lost
possibility of deleting an arbitrary part of the tandem repeat, we add an
additional chain of delete states $D'_0, \dots, D'_{k-2}$, which are accessible
only from state $D_{k-1}$ by transition $D_{k-1}\to D'_0$, and their outgoing
transitions are similar to delete state transitions: $D'_{i}\to M_{i+1},
D'_{i}\to I_i$ for $0\leq i\leq k-2$ and $D'_{i} \to D'_{i+1}$ for $0\leq i <
k-2$. Note that $D'_{k-2}$ is connected only to $M_{k-1}$ to avoid passing a
 full circle by delete states. The full model is in Figure
\ref{SUBFIGURE:SUNFLOWER}. We call this model the \firstUseOf{Sunflower} model.

This model has $4k+1$ states and $12k+1$ transitions, out of which $2k+1$
states are silent. There are $14k+2$ parameters to train for a motif $C$
(including emissions of insert and match states over the alphabet if size $4$).
Models with such a large number of parameters are hard to train, so we reduced
the number of parameters as follows. First, we tied similar transitions, so
that they have the same probability.  We used the set of parameters $p_{ab}$
where $a,b\in \{m, i, d, \cdot\}$, where $m$ stands for any match state, $i$
stands for any insert state, $d$ states for any delete state (from both
chains), and $\cdot$ is either start or final state. Therefore the probability
of transition from match state to delete state is $p_{md}$ and probability of
transition from insert state to final state is $p_{i\cdot}$.  Probabilities
were set so that $\sum_{b\in\{m,i,d\} p_{ab}}=1$ for all possible $a$.
Therefore, transitions from states $M_{k-1}, D_{k-1}$ and $D'_{k-1}$ do not sum
to $1$, because they are either missing one transition or having one additional
transition. For those states, the probability of transitions were normalized in
order to form a probability distribution. 

To reduce the number of emission parameters, all insert states share the same
emission distribution. For the emission distribution of match state $M_i$ we
assumed that the consensus base $c_i$ from the motif evolved over evolutionary
time $t$ according to Jukes-Cantor model\footnote{ The parameter $t$ of
Jukes-Cantor model is not time as measured by years, but rather a branch length
in an evolutionary tree obtained by multiplying of substitution rate and time.
More details can be found in \cite{Durbin1998}.  }. Jukes-Cantor model is a
theoretical model of evolution that assumes constant rate of evolution. Under
this model base $B_1$ evolved over time $t$ to different base $B_2$  with
probability $1/4(1-\exp(-4t/3))$.  The probability that $B_1$ after time $t$
will be still (or again) $B_1$  is $1/4(1+3\exp(-4t/3))$ \cite{Durbin1998}.
Time $t$ was the same for all match states. Therefore emissions of all states
are specified using only $4$ parameters ($1$ for match states and $3$ for
insert states).

To use the sunflower model models for finding tandem repeats, we have to add
state $B$ that models non-repetitive part of the sequence. Let $S_C$ be the
sunflower model for motif $C$. We add the transitions from $B$ to the start
state of $S_C$ and from the final state of $S_C$ to the state $B$. The
probability of transition from the state $B$ to the submodel $S_C$ is $p_r$,
the probability of repeat starting at particular position in the sequence.
Probability of transition from final state of $S_C$ to $B$ is $1$.  There is
also transition $B\to B$ with probability $1-p_r$. We can use the Viterbi
algorithm with this model to find all occurrences of tandem a repeat with motif
$C$. However, with this model we can search only for a specific motif. We call
this method of finding tandem repeats the \abbreviation{Sunflower repeat
finder}{SRF}.

\subsection{TANTAN}\label{SECTION:TANTAN}

\firstUseOf{TANTAN} is a high-order HMM aimed at finding tandem repeat
developed by Frith \cite{Frith2011}. Unlike the Sunflower model, TANTAN models
tandem repeats with an arbitrary motif. Its only restriction is the maximal
length of the motif $K$. In this section, we describe the core of the model;
the part that models tandem repeats. This core can be transformed to the model
usable for search by adding a background state $B$ as we described for the
Sunflower model.

\begin{figure}
\begin{center}
\begin{subfigure}{0.19\textwidth}
\includegraphics{../figures/tantan_simple.pdf}
\caption{Simple model}\label{FIGURE:TANTAN:SIMPLE}
\end{subfigure}%
\begin{subfigure}{0.3\textwidth}
\includegraphics{../figures/tantan_indel.pdf}
\caption{With indels}\label{FIGURE:TANTAN:INDEL}
\end{subfigure}%
\begin{subfigure}{0.35\textwidth}
\includegraphics{../figures/tantan_init.pdf}
\caption{With first repetition}\label{FIGURE:TANTAN:INIT}
\end{subfigure}%
\end{center}
\caption[TANTAN model]{Three variants of the core of the TANTAN model. Gray states are
silent. The first one is a simplified model, only containing repeat states. The second
was used by Frith \cite{Frith2011}, allowing insertions and deletions.  The last one
is our extension with additional prefix states $P_1, \dots P_K$ which model
the first repetition.}\label{FIGURE:TANTAN}
\end{figure}

The principal idea is to use the state of order $l$ to model a repeat with
motif length $l$. TANTANs high order states uses less information than standard
high order states. Emission of symbol $X[i]$ in a standard state of order $l$
depends on subsequence $X[i-l:i]$.  Emission in TANTANs states of order $l$
depends only on the symbol $X[i-l]$. The model consists of $K$ high-order
states $R_l$, $1\leq l\leq K$ called repeat states, where state $R_l$ is of
order $l$. Emission of state $R_l$ is set so that the state emits the same
symbol as $X[i-l]$ with a high probability. By adding transition $R_l\to R_l$
we obtain an HMM modeling tandem repeats without indels with a motif of length
$l$. By connecting states $R_1,\dots, R_K$ to a single start and final state as
shown in Figure \ref{FIGURE:TANTAN:SIMPLE}, we obtain an HMM that models
repeats with the motif lengths $1$ through $K$.

This simplified model does not allow insertions and deletions in repetitions.
Indels are handled similarly to a profile HMMs, by adding insert states
$I_1,\dots, I_{K-1}$ and silent delete states $D_1, \dots, D_{K-1}$ connected
as in the Figure \ref{FIGURE:TANTAN:INDEL}. There are however significant
differences from profile HMMs.  In a profile HMM, delete states are used to
skip at least one match state, while in TANTAN a delete state is used move to a
state with a lower order. Conversely, insert states allows us to move in the
opposite direction; using insert state causes an increase of the order of the
repeat state.  It is also possible to move from the insert state $I_{j}$ to the
insert state $I_{j+1}$, which is not possible in a profile HMM.

One disadvantage of the TANTAN model is that it does not model the first
occurrence of the motif sequence, since repeat states model only the
subsequent repetitions of the sequence. This caused problems when we wanted to
incorporate TANTAN as a submodel for aligning sequences. Therefore we have
added an additional chain of prefix states $P_1,\dots, P_K$ modeling the first
repetition. Transitions from the start state are now only to the final state
(modeling an empty sequence) and the state $P_1$. State $P_l, 1\leq l\leq K$
has transitions to the final state, repeat state $R_l$ and the state $P_{l+1}$,
if such a state exists.

%Emisie a sme hotovy
We set emission distribution of the insert and prefix states to the background
probability: the distribution of the bases in DNA. Emission state of the state
$R_l$ was derived using Jukes-Cantor model with parameter $t$, where the
emission $X[i]$ evolved from $X[i-l]$ over time $t$.

The TANTAN model evolution of repetitions differently than the Sunflower model.
While the Sunflower model assumes that the repetitions evolved independently
from the single common ancestral motif,  TANTAN assumes that each repetition
evolves from the previous repetition. Therefore the mutations, insertions, and
deletions carry over to the following repetition.

\section{Models for Aligning with Tandem Repeats}\label{SECTION:REPMODELS}

In this section we describe models that we have designed and used in our
methods. We describe the model in an dual way; as a generalized pHMM and as an
equivalent non-generalized pHMM (emissions length is at most $1$ in both
sequences). The distribution of generated alignments and annotations (with
certain annotation functions) are same in both models, but clearly the
distribution of state paths will be different due to different sets of states.

The generalized model is obtained by taking the simple 3-state pHMM model from
section \ref{SECTION:ALIGNWITHPHMM} and adding a single generalized pair state
$R$, called the \firstUseOf{repeat state}, which in a single step generates
tandem repeats in both sequences.  The overall topology of GpHMM is illustrated
in the Figure \ref{FIGURE:REPEAT_GENERAL}.

To made this model more flexible, the emission distribution of state $R$ is
defined by an additional pHMM. Since the repetitions in the tandem repeat are
very similar to each other, we did not try to model the evolution of repetitive
parts of the sequences. We assumed that repeat evolved from the original motif
by one event, and then repetitions evolved independently. The goal of state $R$
is to model such evolution in single generalized emission. Model was
constructed from the Sunflower models. Let $C$ be the set of all motifs that
can occur in the alignment. For each motif $c\in C$ we created sunflowers
$S_c^X$ and $S_c^Y$.  Sunflower $S_c^X$ is the sunflower model with motif $c$
which generates symbols only in sequence $X$; $S_c^Y$ generates symbols in $Y$.
We connect $S_c^Y$ and $S_c^X$ by a transition from the final state of $S_c^X$
to the start state of $S_c^Y$ with probability $1$ and thus getting model $H_c$
that generates repeats in both sequences. We connected models $H_c$ for all
$c\in C$ in parallel by a single start state and a single final state as in
Figure \ref{FIGURE:REPEAT_GENERAL}. The probability of the transition from the
start state of $R$ to the start state of model $H_c$ was proportional to the
distribution of motifs $\prob{c}$. Note that the size of this model is
determined mostly by the total length of all motifs, and therefore this model
can be very large, even infinite if we consider all possible sequences as
possible motifs for tandem repeats.  To keep the model size reasonable, we
computed a set of candidate motifs $C$ and use it for construction of the
model. More details are in section \ref{SECTION:REPOPT}. We call this
generalized model the \abbreviation{sunflower field}{SFF} model.

Note that the state $R$ defined as above can generate arbitrary long sequences.
Additionally, the Sunflowers are combined in a way, which does not produce an
alignment of tandem repeats.  Its task is to filter  tandem repeats out of
alignment, so that they do not cause biases in the parts of an alignment
emitted by other states (match and indel states). Alignment of homologous
tandem repeats is done lates in a post-processing step (see Section
\ref{SECTION:ALNDECODING}).

\begin{figure}
\begin{center}
\includegraphics[width=14cm]{../figures/PairRepeatHMMGeneral.pdf}
\end{center}
\caption[General topology of Repeat pHMM]{ 
Topology of the Repeat pHMM. We extended the 3-state pHMM with one generalized
state $R$, which in one emission generates tandem repeats in both sequences.
The emission distribution of state $R$ is defined by another pHMM shown in a
dotted box~(*). Bellow the model we show an example of emitted alignments.
Dashed gray lines corresponds to multiple emissions from the same states, while
the full line represents one emission from a generalized state. 

(*) Black states in this submodel silent start and final states. Spirals
represents sunflower submodels. They are in pairs of two identical models
$S_c^X$ and $S_c^Y$, one for generating tandem repeat in one sequence, other
for generating tandem repeat in the other sequence. There are multiple pairs of
submodels, each for modeling different motif $c$.
}\label{FIGURE:REPEAT_GENERAL} 
\end{figure}

We also experimented with using a TANTAN-like model for defining emissions
distribution of state $R$. We used the TANTAN model with the prefix states from
Section \ref{SECTION:TANTAN}. Similarly as with the sunflower model, we created
two copies of TANTAN, each generating repeats in only one sequence and connect
them together exactly as we would connect the sunflower models. Since TANTAN
model does not rely on a motif, it is not necessary to create more copies of
the TANTAN HMM and its general topology looks like SFF's with only one motif.
Since TANTAN is a high order HMM, the resulting model is a high order and
generalized pair HMM.  We refer to this model as \abbreviation{TANTAN
pHMM}{TTP}. The advantage of this model over SFF is in it's size, since TTP
will have in practice fewer states than SFF; the size of SFF model depends on
the total length of all motif strings while the size of TTP model depends on
the maximum motif length (we set this limit to $50$ bases).  Therefore the SFF
model can be exponentially larger than the TTP.

Another difference between the SFF and the TTP is in the assumed evolution of
orthologous tandem repeats.  The SFF model assumes that repetitions accumulated
mutations independently, while the TTP model assumes that each repetition
evolve from the previous occurrence of the repetition. In the SFF model, we
require that the tandem repeats in both sequences share motif, but there is no
dependence between the two sequences in the TTP model; there is no penalty for
them having different motifs.

In reality, tandem repeats at orthologous positions in two species share common
ancestor and therefore share part of their evolutionary history. However, it is
possible that they were consequently modified by additional evolution events
after speciation (including more tandem duplications). In our model, we ignore
such complex evolution of repetitions, because it would lead to very complex
model and increase the difficulty in decoding and training. Kováč {\it et al.}
developed a method with limited dependence by adding repeat submodels emitting
copies in the two sequences at same time \cite{Kovac2012}.

Both SFF and TTP are defined as $4$ state high order GpHMMs.  In general, using
generalized models increases the time complexity of decoding algorithms
quadratically. Therefore we also used expanded versions of the models: we
replaced the generalized state $R$ with the pHMM submodel defining emissions of
$R$. All transitions entering into $R$ were replaced by transitions to the
start state of the submodel, and all outgoing transitions from $R$ were
replaced by transitions starting in the final state of the submodel. This
transformation does not change the distribution of alignments generated by the
model.  

\subsection{Parameter estimation}\label{SECTION:ALNPAREST}
We used the alignment of the human chromozome 15 and its orthologous sequences
in the dog genome as the \firstUseOf{source alignment} for the estimation of
the parameters of the used models.
In our experiments, we have used $310,091$ consensus sequences found by the TRF
program on the source alignment as motifs for building SFF.  The probability of choosing a particular
motif is the observed frequency of the motif in the TRF output. Since resulting
model is large, we limit Sunflower submodels (see Section \ref{SECTION:REPOPT}.

We set the parameters of the Sunflower submodel manually: the insert and delete
rates were set to $0.005$, the match states allows mutations from motif
sequence according to the Jukes-Cantor model with parameter $t=0.05$. The
emission of the insert states were set according to the frequency of the bases
in the input human-dog alignment.  Parameters of the TANTAN submodel were
estimated by the Baum-Welch algorithm \cite{Durbin1998} on 500 repeats sampled
from the SFF model.

We have annotated the source alignment using TRF program; each alignment column
was annotated as a repeat if any of its bases were annotated as repeat. We
assumed that all columns annotated as repeats were generated by state $R$ and
all other columns were generated by respective state: $M$, $I_X$, or $I_Y$.
Note that is is clear which non-repeat column was generated by which state.
Therefore could estimate the emission distributions and transition
distributions from the frequencies of occurred transitions in transitions in
the source alignment (see Section \ref{SECTION:HMMTRAINING}).

\section{Decoding methods}\label{SECTION:REPDECODING}

The right selection of the decoding method can have significant effect on the
quality\footnote{The term quality is intentionally loose.} of predictions of a
method. In sections \ref{SECTION:SEQUENCEANNOTATION},
\ref{SECTION:OTHERDECODING} and Chapter \ref{CHAPTER:TWOSTAGE}, we described
several decoding algorithms whose aim was to increase the accuracy of the
sequence annotation. Similarly, Lunter {\it et al. (2008)} studied the effect
of different decoding algorithms on the quality of the constructed alignments
(see Section \ref{SECTION:BIASES}) and showed that the Viterbi algorithm was
not the best method for constructing sequence alignments. Therefore it is worth
to consider domain-specific decoding methods for constructing alignments.

In this section we describe several optimization criteria that we used with the
3-state model, the SFF model and the TTP model: the \abbreviation{Viterbi
algorithm}{VA}, the \abbreviation{posterior decoding}{PD}, the
\abbreviation{marginalized posterior decoding}{MPD}, the \abbreviation{block
Viterbi algorithm}{BVA} and the \abbreviation{block posterior decoding}{BPD}.
The first three methods are used with the non-generalized versions of the
models, the latter two methods (block methods) are used with GpHMMs. We have
already described the first three algorithms in Section
\ref{SECTION:ALNDECODING}. While the first three methods are general purpose
decoding methods of pair HMMs, we introduce BPD and BVA as an domain-specific
decoding methods. Their goal is to remove tandem repeats out scope of the three
state part of the SFF model.

Before continuing, we make additional argument for the use of the
domain-specific decoding methods instead of finding the most probable state
path, the most probable annotation, or in case of the pHMM, the most probable
annotation. When constructing the model, we do many simplifications of the
biological processes. This is because because without these simplifications, we
would end up with very large and impractical models. We can use the
domain-specific decoding method as an compensation for such simplifications; we
can practically move the complexity from the model into the decoding method.

In Section \ref{SECTION:HEG} we defined a gain function as a functions of two
annotations. For pair HMMs we define a gain function as a function of two
annotated alignments consisting of alignment columns along with annotation
symbols. This is necessary, because we use generalized models, and therefore
annotation symbols do not uniquely determine the alignment (there can be
multiple alignments with the same annotation).

We represent columns of an annotated alignment using indices to sequences and
annotation symbols. In the case of generalized models, which can generate more
than one column in one step, only the first column will contain annotation
symbol, other columns will contain symbol $\varnothing$. Formally, an
\firstUseOf{annotated alignment} of length $t$ of sequences $X=x_0x_1\dots
x_{n-1}$ and $Y=y_0y_2\dots y_{m-1}$ is represented as a sequence of tuples
$(u_0, a_0, b_0), \dots, (u_{t-1}, a_{t-1}, b_{t-1})$ where $a_i\in \{0, \dots,
{n-1}\}\cup\{-_0, \dots, -_n\}$ and $b_i \in \{0, \dots, m-1\}\cup\{-_0, \dots,
-_m\}$, and $u_i$ is either an annotation symbol or symbol $\varnothing$. We
will refer to triple $(u_i, a_i, v_i)$ as an \firstUseOf{annotated alignment
column}.  Number $i$ represents $i$-th symbol in the corresponding input
sequence and $-_i$ represents a gap in the sequence before position $i$ (if $i$
is the length of the sequence, it represents the gap at the end of the
sequence). For example $(I_X, 47, -_{42})$ means that $x_{47}$ is aligned to a
gap that is between $y_{41}$ and $y_{42}$ and that this column has annotation
$I_X$.  Naturally, indices in $a_i$ and $b_i$ have to be in non-decreasing
order, and each non-dashed symbol has to be in the corresponding sequence in
the alignment exactly once. Additionally, $a_0$ has to be $0$ or $-_{0}$, $a_t$
is $n$ or $-_n$. The constrains for $b$ are analogous.  The first annotation
symbol $u_0\not=\varnothing$ and if some $u_i$ is equal to $\varnothing$ then
such a column was emitted by the same emission as the previous column.  If we
use a non-generalized model, the annotated alignment cannot contain symbol
$\varnothing$. The reason for using annotation symbol $\varnothing$ is that we
need to distinguish between different generalized emissions.  We can define the
probability of an annotated alignment $\Lambda$ as the sum of the probabilities
of the state paths that generate $\Lambda$. We denote this probability as
$\prob{\Lambda\mid X, Y}$.

We are interested in the \firstUseOf{repeat annotation} with annotation
function $\lambda_R$.  For generalized SFF and TTP (and 3-state pHMM),
$\lambda_R$ is the identify function. For the non-generalized versions,
$\lambda_R(u)=R$ for all states $u$ from the submodels modeling tandem repeats.

Similarly as for the regular HMMs, we can define a gain function $G(\Lambda,
\Lambda')$ that corresponds to ``similarity'' of two annotated alignments
$\Lambda$ and $\Lambda'$. Note that for HMMs, symbol $\Lambda$ represents
annotation; with pHMMs, we use this symbol for an annotated alignment.  Since a
pHMM defines the probability distribution $\Lambda_T$ of the correct annotated
alignments, we can define the expected gain of an annotated alignment given
sequence $X$ and $Y$ and gain function $G$: 

\begin{equation}
E_{\Lambda_T\mid X, Y}\left[G(\Lambda_T, \Lambda)\right] = 
\sum_{\Lambda_T}G(\Lambda_T, \Lambda)\prob{\Lambda_T\mid X, Y}
\end{equation}

Since we do not know the correct annotated alignment, we search for the
annotated alignment $\Lambda^*$ with the highest expected gain:

\begin{equation}
\Lambda^* = \arg\max_{\Lambda}
E_{\Lambda_T\mid X, Y}\left[G(\Lambda_T, \Lambda)\right]
\end{equation}

Finally, we express the optimization criteria of various decoding methods using
the highest expected gain framework.

\paragraph{The Viterbi algorithm and block Viterbi algorithm} Gain function for
the Viterbi algorithm assigns $+1$ if the predicted annotated alignment is
identical to the true annotated alignment. In both cases, the labeling function
is the identity function. The optimization algorithms were described in
sections \ref{SECTION:PAIRHMMVITERBI} and \ref{SECTION:GPHMM}. The time
complexity of VA is $O(nmE)$ where $E$ is the number of non-zero transitions in
the model and $n$ and $m$ are lengths of the two sequences. We will discuss the
time complexity of BVA at the end of this section.

We make a distinction between the VA and BVA when using the SFF or the TTP
models. Using the VA on the expanded model is referred to as the VA. Using the
VA on the generalized version of the model will be referred to as the BVA. The
difference between these two versions is that in the VA, we account for only
one state path through the repeat submodel. The BVA however sums all possible
paths through the repeat submodel, and therefore abstracts from the exact
realization of the alignment of tandem repeats to their consensus.

\paragraph{Posterior decoding}
The Viterbi algorithm awards non-zero gain only if an annotated alignment is
entirely correct. The gain function of the posterior decoding is more granular.
In the traditional version (discussed in Section \ref{SECTION:ALNDECODING}) it
assigns $+1$ for every correctly predicted alignment column ignoring column
labels. An annotated alignment column $(u_i, a_i, b_i)$ is correctly predicted
if there is an alignment column $(v, a_i, b_i)$ for arbitrary $v$ in the true
annotated alignment. 

We will consider a stricter version of the PD in which the corresponding
annotated alignment column in the correct annotated alignment has to be
identical (annotations have to be same as well). This stricter condition is
aimed for the expanded versions of SFF and TTP, since without this condition
the PD would treat repeats as gaps despite having orthologs in the other
sequence (our models treat such repeats independently and models them formally
as gaps). For the 3-state HMM, this stricter condition is equivalent to the
original version because each alignment column determine the used state.
The running time of the PD is again $O(nmE)$. 

The difference from the original PD is in the used recurrence. In stricter
version we also keep the used label $c$ in the recurrence $M$. The
interpretation of $M[i,j,a]$ if following.  The score of the best annotated
alignment that aligning sequences $X[:i]$ and $Y[:j]$, where the last
annotation symbol is $a$. The recurrence is following:
\begin{align} 
M[-1,i,a] &= 0, 0\leq i < m\\\
M[i,-1,a] &= 0, 0< i < n\\
M[i,j,a] &= \max_{b\in C}
\begin{cases}
 M[i-1,j-1,b]+\prob{(X[i],a, Y[j])}\\
 M[i,j-1,b]+\prob{(-_{i},a, Y[j])}\\
 M[i-1,j,b]+\prob{(X[i],a, -_{j})}
\end{cases}, 0\leq i<n,0\leq j<m 
\end{align}
Where $C$ is the set of all annotation symbol and $\prob{A}$ is the posterior
probability of the annotation alignment column $A$. The posterior probability
of an annotated alignment column $A$ is the sum of the probabilities of all
annotated alignments that contain $A$. It can be computed by the
Forward-Backward algorithm.

\paragraph{Marginalized posterior decoding}
Marginalized posterior decoding is similar to the posterior decoding. The only
difference is that gaps that differ only in their position in the sequence are
considered identical.  Therefore we treat column $(u, i, -_j)$ to be same as
$(u, i, -_k)$; gaps in the second sequence are treated symmetrically. The
optimization of this gain function is almost identical to the optimization of
posterior decoding. The only difference is that after computation of the
posterior probabilities for all annotated columns, we replace probability $(u, i, -_j)$ with the sum of
$(u, i, -_l)$ for all $l$. The algorithm has also time complexity $O(nmE)$.  As
with PD, this decoding method is not used on the generalized models.

\paragraph{Block posterior decoding}
Block posterior decoding is based on the posterior decoding and is aimed at
generalized models. BPD scores whole emissions instead of individual columns.
The segment of an annotated alignment that was emitted by one emission is
called a \firstUseOf{block}. In particular, the states $M$, $I_X$ and $I_Y$
emit one-column blocks, and state $R$ emits multicolumn blocks. An annotated
alignment can be divided into blocks using $\varnothing$ symbol, which mark
continuation of a block.  The gain function scores each block individually. A
one-column block $(u_i, a_i, b_i), u_i\in\{M, I_X, I_Y\}$ gets score $+1$ if
there is an identical column in the true annotated alignment; otherwise gain
for such a block is $0$. A block of form $\Lambda_E=(u_i, a_i,
b_i)(\varnothing, a_{i+1}, b_{i+1})\dots (\varnothing, a_{j}, b_{j})$ where
$(j+1)$-th column does not contain $\varnothing$ (or there is no $(j+1)$-th
column) gets score $+l$ if the block is correct. The gain $l$ is the number of
emitted non-gap indices in the block. For example block $(R, 4,
-_8)(\varnothing, 5, -_8)(\varnothing, 6, 8)$ contains $4$ non-gap indices: $4,
5, 6$ in the first sequence and $8$ in the second sequence.  The block is
considered correct if exactly the same regions in $X$ and $Y$ form a block with
the same annotation in the true annotated alignment.

\begin{figure}
\begin{center}
\includegraphics[width=11cm]{../figures/BlockPosterior.pdf}
\end{center}
\caption[Block posterior decoding function]{ 
An example of the BPD gain function. Gray solid boxes enclose single blocks,
dashed boxes combine multiple block (each alignment column in such a box is
one block). Block from repeat state have gain $+16$ because it emits $16$
sequences in exactly same regions of the input sequences.  Other alignment
columns get $+1$ only if there is same alignment column in the correct
alignment that was generated by same state.

}\label{FIGURE:BLOCK_POSTERIOR} 
\end{figure}

The reason for using score $+l$ instead of $+1$ is that otherwise the decoding method
would be biased towards short blocks. Instead of $l$, the number of emitted
symbols, we could use the number of emitted columns. However we would then
discriminate the submodels that would tried to align repeats (although we did
not used such models). An example of this gain function in Figure
\ref{FIGURE:BLOCK_POSTERIOR}.

To optimize this gain function, we have to compute posterior probabilities for
all potential blocks. This can be done by the Forward-backward algorithm. In
a GpHMM, each block is given by a state and two intervals; one in
sequence $X$ and one in sequence $Y$. In our models, only state $R$ is
generalized and the overall number of states is constant. The number of
possible blocks is therefore $O(n^2m^2)$. The expected gain of the block 
is its posterior probability multiplied by the number of emitted symbols. After
computing the expected gains of individual blocks, we can compute the highest
scoring annotated alignment in $O(n^2m^2)$ time by the dynamic programming that
is very similar to the Viterbi algorithm from section \ref{SECTION:GPHMM}, but
with 

If we use the Forward algorithm for pHMM for computation of the emissions of
state $R$, then time complexity of the Forward-Backward algorithm (computation
of the posterior probabilities of all blocks) would be in $O(n^3m^3E)$ time,
where $E$ is the number of transitions in the repeat submodel; $O(nmE)$ term is
computing the emission probability. With some pre-processing, we can lower the
$O(nmE)$ term to $O(k)$ where $k$ in the number of sunflower pairs in the
repeat submodel (in case of TTP, $k=1$).

Let us consider the repeat submodel $H$ with $k$ sunflower pairs $S^X_1, S^Y_1,
\dots, S^X_k, S^Y_k$. Let $p_i$ be the probability of entering sunflower
$S^X_i$.  The probability of transition from $S^X_i$ to $S^Y_i$ is $1$. We can
write the probability of generating an arbitrary pair of sequences $X'$ and
$Y'$ by $H$ as

\begin{equation}
\prob{X', Y'\mid H} = \sum_{i=1}^k p_i\prob{X'\mid S^X_i}\prob{Y'\mid S^Y_i}
\end{equation}

By pre-computing all of the emission terms $\prob{X'\mid S^X_i}$ and $\prob{Y'\mid
S^Y_i}$, we can compute the posterior probability of the block in $O(k)$ time.
Naive computation of probability $\prob{X'\mid S^X_i}$ for all intervals $X'$
would take $O(n^3E)$ time, but this can be further improved. When computing
$\prob{X'\mid S^X_i}$ using the forward algorithm, we can use the values from
the forward table to compute the probability of emitting any prefix of $X'$ by
looking at the forward probability of final state for the corresponding column.
We thus need to run the forward algorithm only once for every suffix of $X$.
The optimization for the other sequence is analogous. Using these techniques,
we can pre-compute the emission terms in $O((n^2+m^2)E)$ time, and the overall
time complexity of computing the posterior probabilities of blocks is
$O(kn^2m^2 + (n^2+m^2)E)$. 

Once the posterior probabilities are computed, we find the best-scoring alignment 
using similar dynamic programming as for the PD. The difference is that blocks
now are longer than one column and therefore the recurrence uses   
\begin{align} 
M[-1,i,a] &= 0, 0\leq i < m\\\
M[i,-1,a] &= 0, 0< i < n\\
M[i,j,a] &= \max_{b\in C}
 M[i-i',j-j',b]+\prob{(X[i':i],a, Y[j':j])}
\end{align}
where $0\leq i'\leq i<n,0\leq j'\leq j$ and $i-i'>0$ or $j-j'>0$ (at least one
base was emitted).  The $\prob{(X[i':i],a, Y[j':j])}$ is the posterior
probability of a block with label $a$ containing sequences $X[i':i]$ and
$Y[j':j]$. The time complexity of this step is $O(n^2m^2)$ and therefore the
overall time complexity of BPD is $O(kn^2m^2 + (n^2+m^2)E)$.

We can use the same techniques to optimize the BVA; the most expensive step in
the BVA is the computation of the block probabilities for state $R$, namely
$e_{v,(X',Y')}$. We can use same trick to pre-compute all emission terms in
$O((n^2+m^2))$ time, afterwards we can compute block probability in $O(k)$.
The complexity of the Viterbi algorithm (see Section \ref{SECTION:GPHMM})
will be $O(kn^2m^2)$ and the overall time complexity of BVA is $O(kn^2m^2 +
(n^2+m^2)E)$. 

\paragraph{Post-processing}
The problem with the SFF and the TTP models is that they model repeat sequences
in the sequences $X$ and $Y$ independently and do not try to align tandem
repeats at orthologous locations.  Therefore we post-process the alignment using
the 3-state pHMM by realigning segments of alignments that are annotated as
repeats. We also include adjacent gaps into this post-process step.

\section{Optimizations}\label{SECTION:REPOPT}

In this section, we summarize additional techniques we have used to decrease the
running time of the decoding algorithms. The fastest decoding algorithms described
above run in $O(nmE)$ time, which is still prohibitive for longer sequences.
Additionally, block-based methods are even slower. We used the following
optimizations:

\paragraph{}
We implemented a standard technique of banding (Section \ref{SECTION:SSA}). We
first align input sequences using Muscle \cite{Edgar2004} with default
parameters.  The final alignment methods were then restricted to be within 30
bases from this guide alignment. This technique reduces the $O(nm)$ factor from
the time complexity to $O((n+m)d)$ where $d$ is the maximum distance from the
guide alignment.

\paragraph{}
The size of the SFF model is enormous. It is not practical to use a model with
$310,091$ sunflower pairs. Therefore we used the TRF program \cite{Benson1999}
to find consensus motifs in the input alignment and used only those to build
SFF model. Note that the transition probabilities to individual sunflower pairs
were kept the same as in the original model. In case that TRF found a consensus
that was not in the original set of consensus sequences used to build the SFF
model, we assign it the probability of the least frequent consensus from the
original set. This technique is not applicable to TTP model.

\paragraph{}
To further reduce the running time of the block-based algorithms (BVA and BPD),
we restrict the emissions of the state $R$ to specific intervals in both input
sequences. First, we find the candidate set of tandem repeat intervals for
each input sequence independently. Let $T_X$ and $T_Y$ are the
sets of candidate intervals (not necessarily disjoint) for sequences $X$ and
$Y$ respectively. We restrict emission of state $R$ only to the pairs of
intervals $i_X, i_Y$ where $i_X$ and $i_Y$ have beginning and end within $10$
bases from some interval from $T_X$ and $T_Y$ respectively, or if one of $i_X$
or $i_Y$ is an empty interval (there can be a tandem repeat in only one
sequence).  By this heuristic, state $R$ can have emissions in at most
$(400|T_X|+n)(400|T_Y|+m)$ positions. We applied this method to both SFF and
TPP models.

We used the following algorithm to select candidate intervals $T_X$ and $T_Y$:
\begin{enumerate}[itemsep=-1mm]
\item Run the TRF program on the input sequences $X$ and $Y$. Obtain sets of
intervals $T_X$ and $T_Y$ and lists of consensus sequences $C_X$ and $C_Y$.

\item For every consensus $c\in C_X\cup C_Y$, build the SRF model $S_c$
(sunflower repeat finding model from Section \ref{SECTION:SUNFLOWERMODEL}). 

\item Each model $S_c$ is run on the sequences $X$ and $Y$ using the Viterbi
algorithm. All found repeat intervals are added into corresponding sets $T_X$
and $T_Y$.

\end{enumerate}
The reason for using this method instead of using only intervals from TRF is
that running TRF on the input sequences independently can cause several
problems. For example, if sequence $X$ contains three repetitions of motif $m$
and sequence $Y$ contains only one, then the TRF would return interval only for
sequence $X$. Additionally, consensus can be found in both sequences but can be
rotated, and therefore intervals in both sequences would be shifted.


\section{Experiments}
In sections \ref{SECTION:REPMODELS} and \ref{SECTION:REPDECODING} we proposed
new methods for aligning sequences that contain tandem repeats; SFF and TTP
model.  We also described five decoding methods, out of which two are new (BVA
and BPD). To show the usefulness of the proposed methods, we investigate the
effects of using these methods instead of standard methods for aligning
sequences, which were described in Section \ref{SECTION:REPALNMETHODS}. We 
evaluated these methods on the simulated data generated from a model trained on the human-dog alignment. 
Evaluating model on real data is hard, because we do not know the correct
annotation of the real DNA sequences and we have to use indirect methods for
evaluation. We do not present such comparison in this thesis, but such
comparison can be found in \cite{Nanasi2014}.


\subsection{Simulated data}
We trained the SFF model, the TTP model, and the 3-state HMM on the human-dog
alignment as described in section \ref{SECTION:ALNPAREST}. Then we sampled 200
test alignments each of length at least 200 bases from the SFF submodel with
the condition, that the number of repetitions in the tandem repeat has to be at
least three in both sequences; we were sampling emission from state $R$ until
the condition was met. The reason for this is that otherwise we would have many
segments of alignments that are annotated as repeats, but in fact contain only
one copy of the motif. We refer to these alignments (along with the sampled
repeat annotation) as the correct alignments and correct annotation (repeat
intervals and consensuses).  The program Context \cite{Hickey2011} was trained
of 200 separate alignments sampled from the same SFF model.

\subsection{Accuracy measures}
We use several measures to compare the correct alignment and predicted alignments.
It is important to use multiple measures for evaluating predicted alignments,
because the measure determine the optimal gain function; when comparing using
measure $\beta(\Lambda, \Lambda')$, we can alway search for an alignment that
optimizes the expected value of $\beta(\Lambda, \Lambda')$. 
However in practice we are looking for methods that give good results under 
multiple measures.

The first measure we used was the error rate; the fraction of incorrectly
predicted columns of an alignment. It was measured only on the alignment
columns generated from non-repeat states during sampling, because SFF and TTP
do not model alignment of repetitive regions.

Additionally, we investigate the accuracy of predicting exact repeat block
boundaries in alignments by measuring the number of correctly predicted repeat
blocks; a block is correctly predicted if it contains identical parts of the
sequences as the corresponding block in the correct alignment. We report the
block sensitivity, which is the number of correctly predicted blocks divided by
the number of all repeat blocks in the correct alignment, and block
specificity, which is the number of correctly predicted blocks divided by
the number of all predicted repeat blocks.  We also investigate the accuracy of
prediction of repeat annotation for individual bases; the repeat sensitivity
and specificity.

\subsection{Results}\label{SECTION:REPSIMRESULTS}
We have conducted several experiments to compare
the accuracy of the predicted alignment, and measure the effects of
using different models and algorithms.
%on the accuracy of predicted alignments,
%we did following experiments on the simulated data. 
First we compare the
accuracy of our SFF-based methods with standard alignment methods. 
%Then we
%investigate the effect of using inaccurate intervals and motifs on the accuracy
%of the alignment. Afterwards we compare the SFF model with the TTP model. Then
%we tried to quantify the effect of misspecification of the model parameters,
%and various types of repeat masking. In the end we study the relation between
%the error rate and the distance from the nearest repeat, since we expect that
%the most significant improvement of our model will be near repeats, because
%outside of repeats, the SFF model is very similar to the 3-state model.
%
The results are in the Table
\ref{TABLE:SFFMAIN}. We choose the Viterbi algorithm with the 3-state model as a
baseline method. From standard methods, we also run the Muscle software
\cite{Edgar2004}, the Context \cite{Hickey2011}, and the posterior decoding on the 3-state HMM
with and without hard-masking of tandem repeats.  The hard-masking was done using the TRF
program. None of the standard methods provides repeat annotation, and
therefore we report only the alignment error. The only exception is the PD with
masking, where we annotated predicted columns as repeats if at least one base
in the column was masked.

\def\M{$^\circ$} % mark by a star
\def\MM{$^{\circ\circ\circ}$} % mark by two stars
\def\D{$^{\circ\circ}$} % mark by dagger
\def\DD{$^{\dagger}$} % mark by dagger
\def\R{$^{\yen}$}
\def\RR{$^{\yen\yen}$}
\def\CC#1{\multicolumn{1}{c}{#1}} % center column
\def\S{$^{\star}$}

\begin{table*}
\begin{center}
\begin{tabular}{lr@{\quad}rr@{\quad}rr}
\hline
          & \CC{Alignment} & \multicolumn{2}{c}{Repeat} & 
\multicolumn{2}{c}{Block}\\
Algorithm & \CC{error} & \CC{sn.} & \CC{sp.} & \CC{sn.} & \CC{sp.} \\
\hline
\hline
%\hline
3-state VA (baseline)    & 4.78\% \\
Context             & 5.98\% \\
Muscle              & 5.62\% \\
3-state PD   & \bf 4.41\% \\
3-state masked PD\DD & 5.03\% & 99.23\% & 74.16\% & 7.66\% & 7.24\%\\
\hline
SFF MPD   & {\bf 3.37\%} & {\bf 95.97\%} & 97.78\% & {\bf 43.07\%} & 44.87\%\\
SFF PD    & 3.53\% & 95.86\% & 97.87\% & 42.70\% & 47.37\%\\
SFF BPD   & 3.51\% & 93.09\% & {\bf 98.07}\% & 36.50\% & 41.67\%\\
SFF BVA   & 3.91\% & 93.26\% & 97.96\% & 35.77\% & 40.66\%\\
SFF VA    & 4.04\% & 95.29\% & 97.85\% & 42.70\% & {\bf 48.95\%}\\
\hline
\end{tabular}
\end{center}
\caption[Comparison of the SFF methods]{Accuracy of decoding methods on simulated data. \DD Columns with at
    least one masked character are considered as repeats.
}\label{TABLE:SFFMAIN}
\end{table*}

We run all decoding algorithms described in section \ref{SECTION:ALNDECODING} with SFF model
and compared it with standard alignment methods. 
In general, using the SFF model decreased the alignment error rate by $15-30\%$
compared to the baseline method (see table \ref{TABLE:SFFMAIN}). Other standard
methods also had in general higher error rate than the SFF-based methods.  The
high error rate of the Context program could be caused by insufficient training
data or some software issues. We run muscle with default parameters, and
therefore its scoring scheme was not tailored to our data.  Higher error rate
of PD with hard-masking might be caused by the insufficient accuracy of the
tandem repeat annotation.

The $15\%$ decrease of the error rate was obtained by replacing the 3-state
model with the Sunflower Field model, additional improvements was due to use of
the different decoding algorithms, with the marginalized posterior decoding
having the lowest error rate. The Viterbi based methods had consistently higher
error rate than posterior based methods and the block-based versions of the
algorithms decreased the error rate by at most $3.3\%$ compared to the
non-block based methods. Surprisingly, block based methods had poorest
performance in the block sensitivity and specificity and repeat sensitivity.
These measures are closer to what block methods optimize, so we expected the
opposite. Reason for this is perhaps due to the use of approximate {intervals
obtained by TRF and SRF}. Note that the block-based methods use both
predicted intervals and motifs to restrict the search space due to their high running time. In contrast, the remaining
methods use only motifs (see section \ref{SECTION:REPOPT}). 

\begin{table*}
\begin{center}
\begin{tabular}{lr@{\quad}rr@{\quad}rr}
\hline
          & \CC{Alignment} & \multicolumn{2}{c}{Repeat} & 
\multicolumn{2}{c}{Block}\\
Algorithm & \CC{error} & \CC{sn.} & \CC{sp.} & \CC{sn.} & \CC{sp.} \\
\hline
\hline
%\hline
SFF MPD   & {\bf 3.37\%} & {\bf 95.97\%} & 97.78\% & {\bf 43.07\%} & 44.87\%\\
SFF PD    & 3.53\% & 95.86\% & 97.87\% & 42.70\% & 47.37\%\\
SFF BPD   & 3.51\% & 93.09\% & {\bf 98.07}\% & 36.50\% & 41.67\%\\
SFF BVA   & 3.91\% & 93.26\% & 97.96\% & 35.77\% & 40.66\%\\
SFF VA    & 4.04\% & 95.29\% & 97.85\% & 42.70\% & {\bf 48.95\%}\\
\hline
SFF MPD\M            & {\bf 3.02}\% & {\bf 98.93}\% & 99.64\% & 77.01\% & 76.17\% \\ 
SFF PD\M             & 3.42\% & 98.84\% & 99.51\% & 75.91\% & 80.93\% \\
SFF BPD\MM           & 3.21\% & 97.70\% & \bf 99.87\% & 80.66\% & {\bf 94.44}\% \\
SFF BVA\MM           & 3.71\% & 98.12\% & 99.85\% & {\bf 81.75}\% & 92.18\% \\
SFF VA\M             & 3.94\% & 98.54\% & 99.45\% & 75.55\% & 83.47\% \\
\hline
\end{tabular}
\end{center}
\caption[Effect of using correct intervals]{Accuracy of decoding methods using real motif and/or real intervals on simulated data.
\M method uses motifs from the correct repeat blocks.
\MM method uses the correct motifs and intervals from the
    correct repeat blocks.
}\label{TABLE:SFFMAINORIGINAL}
\end{table*}

Indeed when we replace predicted motifs and intervals with the correct ones,
accuracy of the block-based methods improved significantly (see Table
\ref{TABLE:SFFMAINORIGINAL}). Results in Table \ref{TABLE:SFFMAINORIGINAL}
help us to quantify the effect of not using correct repeat intervals and thus
can be seen as an upper bounds for the performance of our models achievable by
improving repeat annotation. There is clearly room for improvements of repeat
intervals; we could perhaps try to use other programs that detect tandem
repeats, like ATRhunter \cite{Wexler2005} or mreps \cite{Kolpakov2003}.

\begin{table*}
\begin{center}
\begin{tabular}{lr@{\quad}rr@{\quad}rr}
\hline
          & \CC{Alignment} & \multicolumn{2}{c}{Repeat} & 
\multicolumn{2}{c}{Block}\\
Algorithm & \CC{error} & \CC{sn.} & \CC{sp.} & \CC{sn.} & \CC{sp.} \\
\hline
\hline
3-state VA (baseline)    & {4.78}\% \\
\hline
%\hline
SFF BPD   & \bf 3.51\% & 93.09\% & {\bf 98.07}\% & \bf 36.50\% &\bf 41.67\%\\
SFF BVA   & 3.91\% & \bf 93.26\% & 97.96\% & 35.77\% & 40.66\%\\
TTP BPD& 5.05\% & 61.38\% & 97.48\% & 0.00\% & 0.00\%\\
TTP BVA& 6.17\% & 67.86\% & 96.51\% & 0.00\% & 0.00\%\\
\hline
SFF BPD\MM           &\bf 3.21\% & 97.70\% &  99.87\% & 80.66\% & {\bf 94.44}\% \\
SFF BVA\MM           & 3.71\% & \bf 98.12\% & 99.85\% & {\bf 81.75}\% & 92.18\% \\
TTP BPD\D         & 3.42\% & 60.45\% & {\bf 99.90}\% &  0.36\% &  0.46\% \\
TTP BVA\D         & 3.83\% & 61.74\% & 99.88\% & 0.00\% & 0.00\% \\
\hline
\end{tabular}
\end{center}
\caption[Comparison of the TTP method]{Accuracy of decoding methods using real motif and/or real intervals on simulated data.
\D method uses intervals from the correct repeat blocks.
\MM method uses the correct motifs and intervals from the
    correct repeat blocks.
}\label{TABLE:TANTANCOMP}
\end{table*}

The change from predicted to real intervals had even more pronounced effect on
the TTP model (see Table \ref{TABLE:TANTANCOMP}); using the correct intervals,
the TTP model has only slightly worse error rate than the SFF, which is
expectable, since the data were generated from the SFF model. However, using
predicted intervals, TTP has a significantly higher error rate, which means
that TTP is even more sensitive to the intervals that are used. The high error
rate and low repeat and block prediction accuracy could be caused by improperly
modeling the first repetition, because annotations of the TTP methods almost
always skip the first copy of the repeat motif. Additionally, in TPP the
tandem repeat motifs in sequences $X$ and $Y$ are not dependent between sequences, which can  also increase
error rate.  This could be improved by merging the separate chain of prefix
states $P_1, \dots, P_K$ for sequences $X$ and $Y$ into a chain of match states
aligning the first repetition.


Since we use the same model for generating data and testing, we tried to
quantify the effect of misspecification of the model parameters. The SFF model
has two principal types of parameters; parameters of the 3-state HMM and
parameters of the Sunflower model. We investigate the effect of
{misspecification} for both types independently. For the 3-state
model, we have estimated the parameters from human-chicken alignments (chromozome 20 in human genome) instead of
human-dog alignments. For the second type, we have perturbed the each parameter
of the Sunflower model randomly by an additive term from $0.02$ to $0.05$. On these
models, we run marginalized posterior decoding. Table
\ref{TABLE:SFFMARGINALIZED} shows that our method is quite robust. Our method
is more sensitive to change in the training alignments for the 3-state submodel, since the error rate for
MPD increased by $8\%$, but it is still significantly better than the baseline
method. Perturbing parameters for Sunflower submodel did not have significant
effect on error rate (increase by $0.5\%$). 

\begin{table*}
\begin{center}
\begin{tabular}{lr@{\quad}rr@{\quad}rr}
\hline
          & \CC{Alignment} & \multicolumn{2}{c}{Repeat} & 
\multicolumn{2}{c}{Block}\\
Algorithm & \CC{error} & \CC{sn.} & \CC{sp.} & \CC{sn.} & \CC{sp.} \\
\hline
\hline
%\hline
3-state VA (baseline)    & 4.78\% \\
\hline
SFF MPD    & 3.37\% & 95.97\% & 97.78\% & {\bf 43.07\%} & {\bf 44.87}\%\\
SFF MPD\R & 3.63\% & {\bf 96.03\%} & 97.74\% &  42.70\% &  43.33\% \\ 
SFF MPD\RR & {\bf 3.36}\% & 95.99\% & {\bf 97.81}\% & 40.88\% & 43.08\% \\ 
\hline
\end{tabular}
\end{center}
\caption[Test of robustness of the model]{Accuracy for marginalized posterior decoding with different models or real motifs.
\R Parameters of the three-state submodel were estimated from
human-chicken alignment. 
\RR Parameters of SFF submodel were perturbed randomly.
} \label{TABLE:SFFMARGINALIZED}
\end{table*}

We were also investigating the use of the different different repeat finding
methods for masking (see Table \ref{TABLE:SFF3STATEMASK}). We use the Sunflower
model (modified for finding tandem repeats as described in section
\ref{SECTION:SUNFLOWERMODEL}), TRF, and TRF with non-overlapping repeats (we
select maximal scoring non-overlapping set of tandem repeats out of TRF
output). We used hard-masking as described in section
\ref{SECTION:REPALNMETHODS}. For decoding we used both the VA and the PD with the 3-state model.
Apart from  masking the correct intervals, the best performing masking where 
non-overlapping repeats from TRF.

\begin{table*}
\begin{center}
\begin{tabular}{lr@{\quad}rr@{\quad}rr}
\hline
          & \CC{Alignment} & \multicolumn{2}{c}{Repeat} & 
\multicolumn{2}{c}{Block}\\
Algorithm & \CC{error} & \CC{sn.} & \CC{sp.} & \CC{sn.} & \CC{sp.} \\
\hline
\hline
%\hline
3-state Viterbi (baseline)    & 4.78\% \\
\hline
3 state masked PD, TRF\DD       &5.03&{\bf 99.23}&74.16&7.66&7.24\\
3 state masked PD, Sunflower\DD &5.21&98.30&68.65&{\bf 9.12}&7.53\\
3 state masked PD, TRF\S\DD     &{\bf 4.77}&88.17&{\bf 79.63}&7.66&{\bf 9.13}\\
3 state masked PD, \MM\DD       &{\bf 4.50}&{\bf 100.00}&{\bf 97.23}&{\bf 45.99}&{\bf 61.13}\\
\hline
3 state masked VA, TRF\DD         &5.89&{\bf 99.25}&73.29&5.47&5.14\\
3 state masked VA, Sunflower\DD   &5.82&98.32&67.90&{\bf 5.84}&4.89\\
3 state masked VA, TRF\S\DD       &{\bf 5.10}&88.33&{\bf 78.68}&5.47&{\bf 6.47}\\
3 state masked VA, \MM\DD         &{\bf 5.02}&{\bf 100.00}&{\bf 95.41}&{\bf 45.99}&{\bf 44.21}\\
\hline
\end{tabular}
\end{center}
\caption[Comparison of repeat-masking methods]{Accuracy of decoding method using the simple 3-state HMM and hard-masking.
    The best and the second best are bold.  \DD Columns with at least one
    masked character are considered as repeats.
    \MM method uses the correct motifs and intervals from the correct
repeat blocks.  \S Non-overlapping set of repeats from TRF output with maximal
total score is used. 
}\label{TABLE:SFF3STATEMASK} 
\end{table*}

In the last experiment, we investigate the relation between the error rate and
the distance from the nearest repeat.  An alignment column was considered as a
repeat if in the correct alignment at least one base of the column was
annotated as a repeat.  The distance from the repeat is measured by the number
of columns. When the distance from the
repeat is high, our model is almost identical to a 3-state pHMM, and therefore
the performance of both models should be similar in such regions. However, we
do expect models incorporating repeats to be more precise in the regions near
tandem repeats.  Figure \ref{FIGURE:SFF_GRAPHS} contains plots for this metric
for various combinations of parameters. In general, all algorithms have the highest
error rate near boundaries of the repeat, with the error rate lowering with the
distance from the repeat. Most methods reach the baseline error rate within
distance $10$ bases of the repeat border.

\begin{figure}
\begin{center}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{../figures/error_graph_overview.pdf}
\caption{Our methods and traditional methods}\label{FIGURE:SFFOVER}
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{../figures/error_graph_other.pdf}
\caption{Other methods}\label{FIGURE:SFFOTHER}
\end{subfigure}%

\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{../figures/error_graph_sffblock.pdf}
\caption{Block decodings}\label{FIGURE:SFFBLOCKS}
\end{subfigure}%
%\begin{subfigure}{0.5\textwidth}
%\includegraphics[width=\textwidth]{../figures/error_graph_marginalized.pdf}
%\caption{Same method, different models}\label{FIGURE:SFFWEIRD}
%\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{../figures/error_graph_sffvstantan.pdf}
\caption{SFF and TTP}\label{FIGURE:SFFTANTAN}
\end{subfigure}%
%\begin{subfigure}{0.5\textwidth}
%\includegraphics[width=\textwidth]{../figures/error_graph_3statemasking.pdf}
%\caption{Different methods of masking}
%\end{subfigure}%
\caption[Relation between the error rate and distance from the repeat]{
The relation between the error rate and the distance from the nearest repeat. On the Y
axis is error rate of the algorithm; the number of incorrectly aligned columns.
On the X axis is the distance from the nearest repeat. Baseline is the overall
error rate, with no relation to the distance from the repeat border, for
3-state model with the VA.
\M method uses motifs from the correct repeat blocks.
\D method uses intervals from the correct repeat blocks.
\MM method uses the correct motifs and intervals from the
    correct repeat blocks.
\S Non-overlapping set of repeats from TRF output with maximal total
    score is used.}\label{FIGURE:SFF_GRAPHS} 
\end{center}
\end{figure}

Figures \ref{FIGURE:SFFOVER} and \ref{FIGURE:SFFOTHER} suggests that the right
selection of decoding method can improve error rate near repeats, but most of
the drop in the error rate was caused by using the SFF model. As with other
metrics, TTP with correct intervals performs similarly to SFF model, but TTPs
error rate near repeats raises significantly where intervals were obtained from
the TRF (see Figure \ref{FIGURE:SFFTANTAN}).  Figure \ref{FIGURE:SFFBLOCKS}
demonstrates that for block methods, using BVA instead of BPD almost double the error rate
near repeat.  Overall these plots confirm our expectation that the improvement of the SFF
model is concentrated near tandem repeat borders.

